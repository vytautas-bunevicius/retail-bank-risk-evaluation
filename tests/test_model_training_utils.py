"""
Test module for retail_bank_risk model training utilities.

This module contains unit tests for various model training, hyperparameter
optimization, and evaluation functions used in the retail bank risk assessment
project. It uses pytest for test organization and execution.

The module tests the following main functionalities:
1. Data preprocessing (downscaling dtypes, sanitizing feature names)
2. Data validation
3. Model and hyperparameter selection
4. Model training and evaluation
5. Hyperparameter optimization
6. Checkpoint saving and loading
7. Progress tracking
8. Feature importance extraction

Each test function corresponds to a specific utility function from the
retail_bank_risk.model_training_utils module, ensuring that these
functions work as expected with sample data.

Fixtures:
- sample_data_fixture: Provides a sample dataset for testing
- split_sample_data_fixture: Provides train, validation, and test splits of the sample data

The tests cover various scenarios, including:
- Normal operation with valid inputs
- Error handling for invalid inputs or model types
- Compatibility with different model types (XGBoost, LightGBM)
- Checkpoint and progress saving/loading
- Feature importance extraction for different model types

Note: Some tests use mocking to simulate external dependencies and
ensure proper interaction with optimization libraries.
"""

import pytest
import pandas as pd
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from retail_bank_risk.model_training_utils import (
    downscale_dtypes,
    sanitize_feature_names,
    validate_data,
    get_model_and_params,
    train_and_evaluate,
    objective,
    optimize_hyperparameters,
    save_checkpoint,
    load_checkpoint,
    save_progress,
    load_progress,
    extract_feature_importances,
)


@pytest.fixture
def sample_data_fixture():
    """
    Fixture to create a sample dataset for testing.

    Returns:
        tuple: A tuple containing a DataFrame of features (x_df) and a
               Series of target values (y_series).
    """
    x, y = make_classification(n_samples=1000, n_features=20, random_state=42)
    x_df = pd.DataFrame(x, columns=[f"feature_{i}" for i in range(20)])
    y_series = pd.Series(y)
    return x_df, y_series


@pytest.fixture
def split_sample_data_fixture(sample_data_fixture): #pylint: disable=W0621
    """
    Fixture to split sample data into training, validation, and test sets.

    Args:
        sample_data_fixture: The sample dataset generated by `sample_data_fixture` fixture.

    Returns:
        tuple: A tuple containing training, validation, and test splits.
    """
    x, y = sample_data_fixture
    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.2, random_state=42
    )
    x_train, x_val, y_train, y_val = train_test_split(
        x_train, y_train, test_size=0.25, random_state=42
    )
    return x_train, y_train, x_val, y_val, x_test, y_test


def test_downscale_dtypes():
    """
    Test the downscale_dtypes function to ensure data types are properly
    downscaled for both training and test sets.
    """
    df = pd.DataFrame(
        {
            "int_col": np.arange(100, dtype=np.int64),
            "float_col": np.random.rand(100),
            "cat_col": pd.Categorical(["A", "B", "C"] * 33 + ["A"]),
        }
    )

    result = downscale_dtypes(df)

    assert result["int_col"].dtype == np.uint8
    assert result["float_col"].dtype == np.float32
    assert result["cat_col"].dtype == "category"

    df_test = pd.DataFrame(
        {
            "int_col": np.arange(50, 150, dtype=np.int64),
            "float_col": np.random.rand(100) + 1,
            "cat_col": pd.Categorical(["A", "B", "C", "D"] * 25),
        }
    )

    _, result_test = downscale_dtypes(df, df_test)

    assert result_test["int_col"].dtype == np.uint8
    assert result_test["float_col"].dtype == np.float32
    assert result_test["cat_col"].dtype == "category"
    assert set(result_test["cat_col"].cat.categories) == {"A", "B", "C"}


def test_sanitize_feature_names():
    """
    Test the sanitize_feature_names function to ensure feature names are
    properly formatted by replacing invalid characters.
    """
    df = pd.DataFrame(
        {
            "Feature 1": [1, 2, 3],
            "Feature-2": [4, 5, 6],
            "_Feature_3_": [7, 8, 9],
        }
    )

    result = sanitize_feature_names(df)

    assert list(result.columns) == ["Feature_1", "Feature_2", "Feature_3"]


def test_validate_data(sample_data_fixture): #pylint: disable=W0621
    """
    Test the validate_data function to ensure input data validity checks are
    performed correctly.
    """
    x, y = sample_data_fixture

    validate_data(x, y, "test")

    with pytest.raises(AssertionError):
        validate_data(x.iloc[:500], y, "test")

    x_with_null = x.copy()
    x_with_null.iloc[0, 0] = np.nan
    with pytest.raises(AssertionError):
        validate_data(x_with_null, y, "test")


def test_get_model_and_params(mocker):
    """
    Test the get_model_and_params function to ensure models and parameters
    are correctly returned based on the specified model type.
    """
    mock_trial = mocker.Mock()
    mock_trial.suggest_int.return_value = 5
    mock_trial.suggest_loguniform.return_value = 0.1
    mock_trial.suggest_uniform.return_value = 0.8

    xgb_model, xgb_params = get_model_and_params("xgboost", mock_trial, 42, -1)
    assert isinstance(xgb_model(), XGBClassifier)
    assert "max_depth" in xgb_params
    assert "learning_rate" in xgb_params

    lgb_model, lgb_params = get_model_and_params("lightgbm", mock_trial, 42, -1)
    assert isinstance(lgb_model(), LGBMClassifier)
    assert "num_leaves" in lgb_params
    assert "learning_rate" in lgb_params

    with pytest.raises(ValueError):
        get_model_and_params("invalid_model", mock_trial, 42, -1)


def test_train_and_evaluate(split_sample_data_fixture): #pylint: disable=W0621
    """
    Test the train_and_evaluate function to ensure the model can be trained
    and evaluated, and returns an F2 score and selected features.
    """
    x_train, y_train, x_val, y_val, _, _ = split_sample_data_fixture

    model = XGBClassifier(random_state=42)
    f2_score, selected_features = train_and_evaluate(
        model, x_train, y_train, x_val, y_val, 1.0
    )

    assert isinstance(f2_score, float)
    assert 0 <= f2_score <= 1
    assert isinstance(selected_features, list)
    assert all(feature in x_train.columns for feature in selected_features)


def test_objective(split_sample_data_fixture, mocker): #pylint: disable=W0621
    """
    Test the objective function to ensure the optimization objective is
    calculated correctly.
    """
    x_train, y_train, x_val, y_val, _, _ = split_sample_data_fixture

    mock_trial = mocker.Mock()
    mock_trial.suggest_int.return_value = 5
    mock_trial.suggest_loguniform.return_value = 0.1
    mock_trial.suggest_uniform.return_value = 0.8

    f2_score = objective(
        mock_trial, x_train, y_train, x_val, y_val, "xgboost", 42, -1, 1.0
    )

    assert isinstance(f2_score, float)
    assert 0 <= f2_score <= 1


@pytest.mark.parametrize("model_type", ["xgboost", "lightgbm"])
def test_optimize_hyperparameters(
    split_sample_data_fixture, model_type, mocker, tmp_path #pylint: disable=W0621
):
    """
    Test the optimize_hyperparameters function to ensure hyperparameters can
    be optimized and results are returned in the expected format.
    """
    x_train, y_train, x_val, y_val, x_test, y_test = split_sample_data_fixture

    mock_study = mocker.Mock()
    mock_study.best_trial.params = {"max_depth": 3, "learning_rate": 0.1}
    mock_study.best_params = {"max_depth": 3, "learning_rate": 0.1}
    mock_study.trials = [mocker.Mock()] * 5
    mocker.patch("optuna.create_study", return_value=mock_study)

    results = optimize_hyperparameters(
        x_train,
        y_train,
        x_val,
        y_val,
        x_test,
        y_test,
        model_type,
        n_trials=1,
        random_state=42,
        n_jobs=1,
        checkpoint_dir=str(tmp_path),
        study_name="test_study",
        storage="sqlite:///:memory:",
        feature_selection_threshold=1.0,
    )

    assert isinstance(results, dict)
    assert "model" in results
    assert "best_params" in results
    assert "precision" in results
    assert "recall" in results
    assert "f1_score" in results
    assert "f2_score" in results
    assert "auc_roc" in results
    assert "selected_features" in results


def test_save_and_load_checkpoint(tmp_path):
    """
    Test saving and loading model checkpoints to ensure checkpoint data can
    be saved and retrieved without loss.
    """
    data = {"model": XGBClassifier(), "params": {"max_depth": 3}}
    save_checkpoint(data, "test_model", str(tmp_path), is_tuned=True)

    loaded_data = load_checkpoint("test_model", str(tmp_path), is_tuned=True)

    assert isinstance(loaded_data, dict)
    assert "model" in loaded_data
    assert "params" in loaded_data
    assert loaded_data["params"] == data["params"]


def test_save_and_load_progress(mocker):
    """
    Test saving and loading progress to ensure optimization progress can be
    saved and retrieved.
    """
    mock_trial = mocker.Mock()
    y_pred = np.array([0, 1, 1, 0])
    y_pred_proba = np.array([0.2, 0.7, 0.8, 0.3])

    save_progress(mock_trial, "test_model", y_pred, y_pred_proba)

    mock_trial.set_user_attr.assert_called_once()

    mock_trial.user_attrs = {
        "progress": {
            "model": "test_model",
            "y_pred": y_pred.tolist(),
            "y_pred_proba": y_pred_proba.tolist(),
        }
    }

    loaded_progress = load_progress(mock_trial)

    assert isinstance(loaded_progress, dict)
    assert "model" in loaded_progress
    assert "y_pred" in loaded_progress
    assert "y_pred_proba" in loaded_progress
    assert loaded_progress["model"] == "test_model"


def test_extract_feature_importances(sample_data_fixture): #pylint: disable=W0621
    """
    Test extracting feature importances to ensure the correct feature
    importance values are returned for models with and without feature
    importance attributes.
    """
    x, y = sample_data_fixture

    model_with_importances = XGBClassifier(random_state=42)
    model_with_importances.fit(x, y)
    importances = extract_feature_importances(model_with_importances, x, y)

    assert isinstance(importances, np.ndarray)
    assert len(importances) == x.shape[1]

    class DummyModel:
        """Dummy model without feature_importances_ attribute."""

        def fit(self, x, y):
            """
            Fit the model to the data.

            Args:
                x: Input features.
                y: Target variable.
            """
            pass #pylint: disable=W0107

        def predict(self, x):
            """
            Predict using the model.

            Args:
                x: Input features.

            Returns:
                np.ndarray: Array of zeros as dummy predictions.
            """
            return np.zeros(x.shape[0])

        def score(self, x, y): #pylint: disable=W0613
            """Return a fixed score for simplicity."""
            return 0.5

    model_without_importances = DummyModel()
    model_without_importances.fit(x, y)
    importances = extract_feature_importances(model_without_importances, x, y)

    assert isinstance(importances, np.ndarray)
    assert len(importances) == x.shape[1]

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "This notebook builds upon the feature-engineered dataset from the previous notebook (`04_feature_engineering.ipynb`) and focuses on **Model Training and Evaluation**. Our primary goal is to develop a credit risk prediction model that excels at identifying potential loan defaulters, thereby minimizing financial losses for retail banks while also considering their desired balance between risk aversion and loan approval rates. This translates to maximizing the recall of the positive class (loan defaulters) while maintaining acceptable precision and overall model performance.\n",
    "\n",
    "### 0.5.1 Objectives\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "1. **Model Selection:** Choose algorithms suitable for imbalanced classification problems.\n",
    "2. **Model Training:** Train models with a focus on identifying potential defaulters.\n",
    "3. **Hyperparameter Tuning:** Optimize models to increase recall for the positive class.\n",
    "4. **Model Evaluation:** Assess models primarily on recall, while considering precision, F2-score, AUC-PR, and overall performance.\n",
    "5. **Model Comparison:** Compare different models based on their ability to identify true positives and balance the precision-recall trade-off.\n",
    "6. **Threshold Adjustment:** Explore the impact of classification thresholds on recall and precision, collaborating with retail banks to determine the optimal threshold.\n",
    "\n",
    "### 0.5.2 Importance of Focusing on Recall\n",
    "\n",
    "Prioritizing recall for defaulter prediction is crucial for minimizing financial losses, which is the primary business objective in credit risk assessment. The cost of missing a potential defaulter (false negative) is typically much higher than the cost of incorrectly classifying a non-defaulter as high-risk (false positive). While we prioritize recall, we will also carefully consider the precision-recall trade-off and aim for a model that maximizes recall without severely impacting precision. Techniques like threshold adjustment and cost-sensitive learning will be used to balance these metrics effectively. Furthermore, demonstrating a thorough approach to risk identification aligns with regulatory expectations in the financial sector, supporting the banks' compliance needs. This approach also allows for more conservative lending practices, which can be adjusted based on the bank's specific risk tolerance.\n",
    "\n",
    "### 0.5.3 Our Approach\n",
    "\n",
    "In this notebook, we will focus on the following modeling tasks:\n",
    "\n",
    "1. **Data Preparation:** Address class imbalance using techniques like SMOTE or class weighting.\n",
    "2. **Baseline Model:** A logistic regression model with class weights inversely proportional to class frequencies will serve as our baseline. This will provide a benchmark for evaluating more complex models.\n",
    "3. **Advanced Models:** Train and evaluate models known for handling imbalanced data:\n",
    "   - Decision Trees with adjusted class weights\n",
    "   - Random Forest with balanced class weights\n",
    "   - Gradient Boosting (XGBoost, LightGBM) with `scale_pos_weight` adjustment\n",
    "4. **Hyperparameter Tuning:** We will employ techniques like GridSearchCV or RandomizedSearchCV, optimizing for the F2-score (which gives more weight to recall) or a custom cost-sensitive scoring function.\n",
    "5. **Model Evaluation:** Prioritize recall in our metrics, while also considering precision, F2-score, AUC-PR, and AUC-ROC.\n",
    "6. **Threshold Adjustment:** We will experiment with different classification thresholds and work closely with retail banks to determine the optimal threshold that balances their desired level of risk aversion with acceptable loan approval rates.\n",
    "7. **Ensemble Methods:** Explore ensemble techniques that can improve recall without severely impacting precision.\n",
    "8. **Cost-Sensitive Learning:** Incorporate misclassification costs to reflect the higher cost of false negatives, aligning the model's objective with the business goal of minimizing financial losses.\n",
    "\n",
    "By the end of this notebook, we aim to have a model (or ensemble of models) that excels at identifying potential loan defaulters, providing the bank with a powerful tool for risk assessment and mitigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from retail_bank_risk.model_training_utils import downscale_dtypes\n",
    "from retail_bank_risk.advanced_visualizations_utils import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_model_performance,\n",
    "    shap_summary_plot,\n",
    "    shap_force_plot,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_combined_confusion_matrices,\n",
    "    plot_learning_curve,\n",
    ")\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the necessary libraries, we load the training and test datasets and verify their dimensions and initial rows to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\n",
    "    \"../data/processed/application_train_engineered.parquet\"\n",
    ")\n",
    "test_df = pd.read_parquet(\n",
    "    \"../data/processed/application_test_engineered.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (307511, 78)\n",
      "Test Data Shape: (48744, 77)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg_city_not_work_city_0</th>\n",
       "      <th>reg_city_not_work_city_1</th>\n",
       "      <th>region_rating_client_w_city</th>\n",
       "      <th>region_rating_client</th>\n",
       "      <th>name_contract_type_cash loans</th>\n",
       "      <th>name_contract_type_revolving loans</th>\n",
       "      <th>code_gender_m</th>\n",
       "      <th>code_gender_f</th>\n",
       "      <th>flag_own_car_n</th>\n",
       "      <th>flag_own_car_y</th>\n",
       "      <th>...</th>\n",
       "      <th>is_anomaly_true</th>\n",
       "      <th>age_group</th>\n",
       "      <th>income_group</th>\n",
       "      <th>credit_amount_group</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>credit_to_goods_ratio</th>\n",
       "      <th>annuity_to_income_ratio</th>\n",
       "      <th>ext_source_mean</th>\n",
       "      <th>credit_exceeds_goods</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.007889</td>\n",
       "      <td>1.158397</td>\n",
       "      <td>0.121978</td>\n",
       "      <td>0.201162</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.790750</td>\n",
       "      <td>1.145199</td>\n",
       "      <td>0.132217</td>\n",
       "      <td>0.587418</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.642739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.316167</td>\n",
       "      <td>1.052803</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.579984</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.179963</td>\n",
       "      <td>0.381898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   reg_city_not_work_city_0  reg_city_not_work_city_1  \\\n",
       "0                         1                         0   \n",
       "1                         1                         0   \n",
       "2                         1                         0   \n",
       "3                         1                         0   \n",
       "4                         0                         1   \n",
       "\n",
       "   region_rating_client_w_city  region_rating_client  \\\n",
       "0                          1.0                   1.0   \n",
       "1                          2.0                   2.0   \n",
       "2                          1.0                   1.0   \n",
       "3                          1.0                   1.0   \n",
       "4                          1.0                   1.0   \n",
       "\n",
       "   name_contract_type_cash loans  name_contract_type_revolving loans  \\\n",
       "0                              1                                   0   \n",
       "1                              1                                   0   \n",
       "2                              0                                   1   \n",
       "3                              1                                   0   \n",
       "4                              1                                   0   \n",
       "\n",
       "   code_gender_m  code_gender_f  flag_own_car_n  flag_own_car_y  ...  \\\n",
       "0              1              0               1               0  ...   \n",
       "1              0              1               1               0  ...   \n",
       "2              1              0               0               1  ...   \n",
       "3              0              1               1               0  ...   \n",
       "4              1              0               1               0  ...   \n",
       "\n",
       "   is_anomaly_true  age_group  income_group  credit_amount_group  \\\n",
       "0                0        1.0           3.0                  1.0   \n",
       "1                0        3.0           4.0                  4.0   \n",
       "2                0        3.0           0.0                  0.0   \n",
       "3                0        3.0           1.0                  1.0   \n",
       "4                0        3.0           1.0                  2.0   \n",
       "\n",
       "   debt_to_income_ratio  credit_to_goods_ratio  annuity_to_income_ratio  \\\n",
       "0              2.007889               1.158397                 0.121978   \n",
       "1              4.790750               1.145199                 0.132217   \n",
       "2              2.000000               1.000000                 0.100000   \n",
       "3              2.316167               1.052803                 0.219900   \n",
       "4              4.222222               1.000000                 0.179963   \n",
       "\n",
       "   ext_source_mean  credit_exceeds_goods  target  \n",
       "0         0.201162                     1       1  \n",
       "1         0.587418                     1       0  \n",
       "2         0.642739                     0       0  \n",
       "3         0.579984                     1       0  \n",
       "4         0.381898                     0       0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Training Data Shape: {train_df.shape}\")\n",
    "print(f\"Test Data Shape: {test_df.shape}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data appears correctly processed: encoded variables are saved, new features are created, and the dataset shapes match expectations.\n",
    "\n",
    "Next, we optimize data types to minimize memory usage without information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Data columns (total 78 columns):\n",
      " #   Column                                   Non-Null Count   Dtype  \n",
      "---  ------                                   --------------   -----  \n",
      " 0   reg_city_not_work_city_0                 307511 non-null  uint8  \n",
      " 1   reg_city_not_work_city_1                 307511 non-null  uint8  \n",
      " 2   region_rating_client_w_city              307511 non-null  float32\n",
      " 3   region_rating_client                     307511 non-null  float32\n",
      " 4   name_contract_type_cash loans            307511 non-null  uint8  \n",
      " 5   name_contract_type_revolving loans       307511 non-null  uint8  \n",
      " 6   code_gender_m                            307511 non-null  uint8  \n",
      " 7   code_gender_f                            307511 non-null  uint8  \n",
      " 8   flag_own_car_n                           307511 non-null  uint8  \n",
      " 9   flag_own_car_y                           307511 non-null  uint8  \n",
      " 10  flag_own_realty_y                        307511 non-null  uint8  \n",
      " 11  flag_own_realty_n                        307511 non-null  uint8  \n",
      " 12  name_type_suite_unaccompanied            307511 non-null  uint8  \n",
      " 13  name_type_suite_family                   307511 non-null  uint8  \n",
      " 14  name_type_suite_spouse, partner          307511 non-null  uint8  \n",
      " 15  name_type_suite_children                 307511 non-null  uint8  \n",
      " 16  name_type_suite_other_a                  307511 non-null  uint8  \n",
      " 17  name_type_suite_mode                     307511 non-null  uint8  \n",
      " 18  name_type_suite_other_b                  307511 non-null  uint8  \n",
      " 19  name_type_suite_group of people          307511 non-null  uint8  \n",
      " 20  name_income_type_working                 307511 non-null  uint8  \n",
      " 21  name_income_type_state servant           307511 non-null  uint8  \n",
      " 22  name_income_type_commercial associate    307511 non-null  uint8  \n",
      " 23  name_income_type_pensioner               307511 non-null  uint8  \n",
      " 24  name_income_type_unemployed              307511 non-null  uint8  \n",
      " 25  name_income_type_student                 307511 non-null  uint8  \n",
      " 26  name_income_type_businessman             307511 non-null  uint8  \n",
      " 27  name_income_type_maternity leave         307511 non-null  uint8  \n",
      " 28  name_education_type                      307511 non-null  float32\n",
      " 29  name_family_status_single / not married  307511 non-null  uint8  \n",
      " 30  name_family_status_married               307511 non-null  uint8  \n",
      " 31  name_family_status_civil marriage        307511 non-null  uint8  \n",
      " 32  name_family_status_widow                 307511 non-null  uint8  \n",
      " 33  name_family_status_separated             307511 non-null  uint8  \n",
      " 34  name_family_status_unknown               307511 non-null  uint8  \n",
      " 35  name_housing_type_house / apartment      307511 non-null  uint8  \n",
      " 36  name_housing_type_rented apartment       307511 non-null  uint8  \n",
      " 37  name_housing_type_with parents           307511 non-null  uint8  \n",
      " 38  name_housing_type_municipal apartment    307511 non-null  uint8  \n",
      " 39  name_housing_type_office apartment       307511 non-null  uint8  \n",
      " 40  name_housing_type_co-op apartment        307511 non-null  uint8  \n",
      " 41  occupation_type                          307511 non-null  float32\n",
      " 42  weekday_appr_process_start_wednesday     307511 non-null  uint8  \n",
      " 43  weekday_appr_process_start_monday        307511 non-null  uint8  \n",
      " 44  weekday_appr_process_start_thursday      307511 non-null  uint8  \n",
      " 45  weekday_appr_process_start_sunday        307511 non-null  uint8  \n",
      " 46  weekday_appr_process_start_saturday      307511 non-null  uint8  \n",
      " 47  weekday_appr_process_start_friday        307511 non-null  uint8  \n",
      " 48  weekday_appr_process_start_tuesday       307511 non-null  uint8  \n",
      " 49  organization_type                        307511 non-null  float32\n",
      " 50  housetype_mode_block of flats            307511 non-null  uint8  \n",
      " 51  housetype_mode_mode                      307511 non-null  uint8  \n",
      " 52  housetype_mode_terraced house            307511 non-null  uint8  \n",
      " 53  housetype_mode_specific housing          307511 non-null  uint8  \n",
      " 54  emergencystate_mode_no                   307511 non-null  uint8  \n",
      " 55  emergencystate_mode_mode                 307511 non-null  uint8  \n",
      " 56  emergencystate_mode_yes                  307511 non-null  uint8  \n",
      " 57  days_last_phone_change                   307511 non-null  float32\n",
      " 58  days_birth                               307511 non-null  float32\n",
      " 59  days_id_publish                          307511 non-null  float32\n",
      " 60  ext_source_3                             307511 non-null  float32\n",
      " 61  ext_source_2                             307511 non-null  float32\n",
      " 62  sk_id_curr                               307511 non-null  float32\n",
      " 63  amt_income_total                         307511 non-null  float32\n",
      " 64  amt_credit                               307511 non-null  float32\n",
      " 65  amt_annuity                              307511 non-null  float32\n",
      " 66  amt_goods_price                          307511 non-null  float32\n",
      " 67  is_anomaly_false                         307511 non-null  uint8  \n",
      " 68  is_anomaly_true                          307511 non-null  uint8  \n",
      " 69  age_group                                307511 non-null  float32\n",
      " 70  income_group                             307511 non-null  float32\n",
      " 71  credit_amount_group                      307511 non-null  float32\n",
      " 72  debt_to_income_ratio                     307511 non-null  float32\n",
      " 73  credit_to_goods_ratio                    307511 non-null  float32\n",
      " 74  annuity_to_income_ratio                  307511 non-null  float32\n",
      " 75  ext_source_mean                          307511 non-null  float32\n",
      " 76  credit_exceeds_goods                     307511 non-null  uint8  \n",
      " 77  target                                   307511 non-null  uint8  \n",
      "dtypes: float32(22), uint8(56)\n",
      "memory usage: 42.2 MB\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = downscale_dtypes(train_df, test_df, target_column='target')\n",
    "\n",
    "train_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be splitting the dataset into predictors and the target variable for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 76\n",
      "Feature Names: ['reg_city_not_work_city_0', 'reg_city_not_work_city_1', 'region_rating_client_w_city', 'region_rating_client', 'name_contract_type_cash loans', 'name_contract_type_revolving loans', 'code_gender_m', 'code_gender_f', 'flag_own_car_n', 'flag_own_car_y', 'flag_own_realty_y', 'flag_own_realty_n', 'name_type_suite_unaccompanied', 'name_type_suite_family', 'name_type_suite_spouse, partner', 'name_type_suite_children', 'name_type_suite_other_a', 'name_type_suite_mode', 'name_type_suite_other_b', 'name_type_suite_group of people', 'name_income_type_working', 'name_income_type_state servant', 'name_income_type_commercial associate', 'name_income_type_pensioner', 'name_income_type_unemployed', 'name_income_type_student', 'name_income_type_businessman', 'name_income_type_maternity leave', 'name_education_type', 'name_family_status_single / not married', 'name_family_status_married', 'name_family_status_civil marriage', 'name_family_status_widow', 'name_family_status_separated', 'name_family_status_unknown', 'name_housing_type_house / apartment', 'name_housing_type_rented apartment', 'name_housing_type_with parents', 'name_housing_type_municipal apartment', 'name_housing_type_office apartment', 'name_housing_type_co-op apartment', 'occupation_type', 'weekday_appr_process_start_wednesday', 'weekday_appr_process_start_monday', 'weekday_appr_process_start_thursday', 'weekday_appr_process_start_sunday', 'weekday_appr_process_start_saturday', 'weekday_appr_process_start_friday', 'weekday_appr_process_start_tuesday', 'organization_type', 'housetype_mode_block of flats', 'housetype_mode_mode', 'housetype_mode_terraced house', 'housetype_mode_specific housing', 'emergencystate_mode_no', 'emergencystate_mode_mode', 'emergencystate_mode_yes', 'days_last_phone_change', 'days_birth', 'days_id_publish', 'ext_source_3', 'ext_source_2', 'amt_income_total', 'amt_credit', 'amt_annuity', 'amt_goods_price', 'is_anomaly_false', 'is_anomaly_true', 'age_group', 'income_group', 'credit_amount_group', 'debt_to_income_ratio', 'credit_to_goods_ratio', 'annuity_to_income_ratio', 'ext_source_mean', 'credit_exceeds_goods']\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop([\"target\", \"sk_id_curr\"], axis=1)\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "print(f\"Number of Features: {X.shape[1]}\")\n",
    "print(\"Feature Names:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features in test set: 76\n",
      "Feature Names in test set: ['reg_city_not_work_city_0', 'reg_city_not_work_city_1', 'region_rating_client_w_city', 'region_rating_client', 'name_contract_type_cash loans', 'name_contract_type_revolving loans', 'code_gender_m', 'code_gender_f', 'flag_own_car_n', 'flag_own_car_y', 'flag_own_realty_y', 'flag_own_realty_n', 'name_type_suite_unaccompanied', 'name_type_suite_family', 'name_type_suite_spouse, partner', 'name_type_suite_children', 'name_type_suite_other_a', 'name_type_suite_mode', 'name_type_suite_other_b', 'name_type_suite_group of people', 'name_income_type_working', 'name_income_type_state servant', 'name_income_type_commercial associate', 'name_income_type_pensioner', 'name_income_type_unemployed', 'name_income_type_student', 'name_income_type_businessman', 'name_income_type_maternity leave', 'name_education_type', 'name_family_status_single / not married', 'name_family_status_married', 'name_family_status_civil marriage', 'name_family_status_widow', 'name_family_status_separated', 'name_family_status_unknown', 'name_housing_type_house / apartment', 'name_housing_type_rented apartment', 'name_housing_type_with parents', 'name_housing_type_municipal apartment', 'name_housing_type_office apartment', 'name_housing_type_co-op apartment', 'occupation_type', 'weekday_appr_process_start_wednesday', 'weekday_appr_process_start_monday', 'weekday_appr_process_start_thursday', 'weekday_appr_process_start_sunday', 'weekday_appr_process_start_saturday', 'weekday_appr_process_start_friday', 'weekday_appr_process_start_tuesday', 'organization_type', 'housetype_mode_block of flats', 'housetype_mode_mode', 'housetype_mode_terraced house', 'housetype_mode_specific housing', 'emergencystate_mode_no', 'emergencystate_mode_mode', 'emergencystate_mode_yes', 'days_last_phone_change', 'days_birth', 'days_id_publish', 'ext_source_3', 'ext_source_2', 'amt_income_total', 'amt_credit', 'amt_annuity', 'amt_goods_price', 'is_anomaly_false', 'is_anomaly_true', 'age_group', 'income_group', 'credit_amount_group', 'debt_to_income_ratio', 'credit_to_goods_ratio', 'annuity_to_income_ratio', 'ext_source_mean', 'credit_exceeds_goods']\n"
     ]
    }
   ],
   "source": [
    "X_test = test_df.drop(\"sk_id_curr\", axis=1)\n",
    "sk_id_curr = test_df[\"sk_id_curr\"]\n",
    "\n",
    "print(f\"Number of Features in test set: {X_test.shape[1]}\")\n",
    "print(\"Feature Names in test set:\", X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we divide the data to evaluate model performance on unseen data while preserving class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (246008, 76)\n",
      "Validation Set Shape: (61503, 76)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training Set Shape: {X_train.shape}\")\n",
    "print(f\"Validation Set Shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before optimizing and tuning complex models, it's essential to establish baseline models.\n",
    "\n",
    "These models will provide a benchmark against which you can measure the performance of more sophisticated models and tuning strategies.\n",
    "\n",
    "We will start by defining the baseline pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_feature_names(X):\n",
    "    return X.rename(columns=lambda x: re.sub(r'[^\\w]+', '_', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "sanitize_transformer = FunctionTransformer(sanitize_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dummy = Pipeline([\n",
    "    ('sanitize', sanitize_transformer),\n",
    "    ('classifier', DummyClassifier(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('sanitize', sanitize_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline_dt = Pipeline([\n",
    "    ('sanitize', sanitize_transformer),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('sanitize', sanitize_transformer),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('sanitize', sanitize_transformer),\n",
    "    ('classifier', xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_lgb = Pipeline([\n",
    "    ('sanitize', sanitize_transformer),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will organize models into a list for streamlined training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_models = [\n",
    "    ('Dummy Classifier', pipeline_dummy),\n",
    "    ('Logistic Regression', pipeline_lr),\n",
    "    ('Decision Tree', pipeline_dt),\n",
    "    ('Random Forest', pipeline_rf),\n",
    "    ('XGBoost', pipeline_xgb),\n",
    "    ('LightGBM', pipeline_lgb)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run the function which will automate the training and evaluation process, providing key metrics and visualizations for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dummy Classifier...\n",
      "Training Logistic Regression...\n",
      "\n",
      "Classification Report for Dummy Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     56538\n",
      "           1       0.00      0.00      0.00      4965\n",
      "\n",
      "    accuracy                           0.92     61503\n",
      "   macro avg       0.46      0.50      0.48     61503\n",
      "weighted avg       0.85      0.92      0.88     61503\n",
      "\n",
      "Confusion Matrix for Dummy Classifier:\n",
      "[[56538     0]\n",
      " [ 4965     0]]\n",
      "\n",
      "Dummy Classifier Metrics:\n",
      "Recall: 0.0000\n",
      "Precision: 0.0000\n",
      "F1 Score: 0.0000\n",
      "AUC-PR: 0.5404\n",
      "AUC-ROC: 0.5000\n",
      "\n",
      "Training Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vytautasbunevicius/retail-bank-risk-evaluation/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/vytautasbunevicius/retail-bank-risk-evaluation/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/vytautasbunevicius/retail-bank-risk-evaluation/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Training XGBoost...\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3532\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
      "[LightGBM] [Info] Start training from score -2.432482\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     56538\n",
      "           1       0.52      0.01      0.02      4965\n",
      "\n",
      "    accuracy                           0.92     61503\n",
      "   macro avg       0.72      0.50      0.49     61503\n",
      "weighted avg       0.89      0.92      0.88     61503\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[56489    49]\n",
      " [ 4912    53]]\n",
      "\n",
      "Logistic Regression Metrics:\n",
      "Recall: 0.0107\n",
      "Precision: 0.5196\n",
      "F1 Score: 0.0209\n",
      "AUC-PR: 0.2238\n",
      "AUC-ROC: 0.7432\n",
      "\n",
      "Confusion matrix saved to ../images/model_performance/dummy_classifier_confusion_matrix.png\n",
      "ROC curve saved to ../images/model_performance/dummy_classifier_roc_curve.png\n",
      "Precision-Recall curve saved to ../images/model_performance/dummy_classifier_pr_curve.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vytautasbunevicius/retail-bank-risk-evaluation/venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [19:58:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to ../images/model_performance/logistic_regression_confusion_matrix.png\n",
      "ROC curve saved to ../images/model_performance/logistic_regression_roc_curve.png\n",
      "Precision-Recall curve saved to ../images/model_performance/logistic_regression_pr_curve.png\n",
      "Learning curve saved to ../images/model_performance/dummy_classifier_learning_curve.png\n",
      "============================================================\n",
      "\n",
      "\n",
      "Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56538\n",
      "           1       1.00      1.00      1.00      4965\n",
      "\n",
      "    accuracy                           1.00     61503\n",
      "   macro avg       1.00      1.00      1.00     61503\n",
      "weighted avg       1.00      1.00      1.00     61503\n",
      "\n",
      "Confusion Matrix for Decision Tree:\n",
      "[[56526    12]\n",
      " [    8  4957]]\n",
      "\n",
      "Decision Tree Metrics:\n",
      "Recall: 0.9984\n",
      "Precision: 0.9976\n",
      "F1 Score: 0.9980\n",
      "AUC-PR: 0.9981\n",
      "AUC-ROC: 0.9991\n",
      "\n",
      "Confusion matrix saved to ../images/model_performance/decision_tree_confusion_matrix.png\n",
      "ROC curve saved to ../images/model_performance/decision_tree_roc_curve.png\n",
      "Precision-Recall curve saved to ../images/model_performance/decision_tree_pr_curve.png\n",
      "\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56538\n",
      "           1       1.00      1.00      1.00      4965\n",
      "\n",
      "    accuracy                           1.00     61503\n",
      "   macro avg       1.00      1.00      1.00     61503\n",
      "weighted avg       1.00      1.00      1.00     61503\n",
      "\n",
      "Confusion Matrix for XGBoost:\n",
      "[[56538     0]\n",
      " [    0  4965]]\n",
      "\n",
      "XGBoost Metrics:\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "F1 Score: 1.0000\n",
      "AUC-PR: 1.0000\n",
      "AUC-ROC: 1.0000\n",
      "\n",
      "Learning curve saved to ../images/model_performance/logistic_regression_learning_curve.png\n",
      "============================================================\n",
      "\n",
      "Confusion matrix saved to ../images/model_performance/xgboost_confusion_matrix.png\n",
      "ROC curve saved to ../images/model_performance/xgboost_roc_curve.png\n",
      "Precision-Recall curve saved to ../images/model_performance/xgboost_pr_curve.png\n",
      "\n",
      "Classification Report for LightGBM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56538\n",
      "           1       1.00      1.00      1.00      4965\n",
      "\n",
      "    accuracy                           1.00     61503\n",
      "   macro avg       1.00      1.00      1.00     61503\n",
      "weighted avg       1.00      1.00      1.00     61503\n",
      "\n",
      "Confusion Matrix for LightGBM:\n",
      "[[56538     0]\n",
      " [    0  4965]]\n",
      "\n",
      "LightGBM Metrics:\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "F1 Score: 1.0000\n",
      "AUC-PR: 1.0000\n",
      "AUC-ROC: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:58:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:58:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:58:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:58:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Random Forest:\n",
      "Confusion matrix saved to ../images/model_performance/lightgbm_confusion_matrix.png\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97     56538\n",
      "           1       1.00      0.35      0.52      4965\n",
      "\n",
      "    accuracy                           0.95     61503\n",
      "   macro avg       0.97      0.68      0.75     61503\n",
      "weighted avg       0.95      0.95      0.94     61503\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "[[56532     6]\n",
      " [ 3214  1751]]\n",
      "\n",
      "Random Forest Metrics:\n",
      "Recall: 0.3527\n",
      "Precision: 0.9966\n",
      "F1 Score: 0.5210\n",
      "AUC-PR: 0.8713\n",
      "AUC-ROC: 0.9753\n",
      "\n",
      "ROC curve saved to ../images/model_performance/lightgbm_roc_curve.png\n",
      "Precision-Recall curve saved to ../images/model_performance/lightgbm_pr_curve.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:58:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:58:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1577, number of negative: 18103\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 18156\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3474\n",
      "[LightGBM] [Info] Number of data points in the train set: 19680, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080132 -> initscore=-2.440553\n",
      "[LightGBM] [Info] Start training from score -2.440553\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3476\n",
      "[LightGBM] [Info] Number of data points in the train set: 19680, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.077439 -> initscore=-2.477663\n",
      "[LightGBM] [Info] Start training from score -2.477663\n",
      "[LightGBM] [Info] Number of positive: 5159, number of negative: 58802\n",
      "[LightGBM] [Info] Number of positive: 5159, number of negative: 58802\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3510\n",
      "[LightGBM] [Info] Number of data points in the train set: 63961, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080659 -> initscore=-2.433433\n",
      "[LightGBM] [Info] Start training from score -2.433433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:58:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3507\n",
      "[LightGBM] [Info] Number of data points in the train set: 63961, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080659 -> initscore=-2.433433\n",
      "[LightGBM] [Info] Start training from score -2.433433\n",
      "[LightGBM] [Info] Number of positive: 8669, number of negative: 99574\n",
      "[LightGBM] [Info] Number of positive: 8669, number of negative: 99574\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3520\n",
      "[LightGBM] [Info] Number of data points in the train set: 108243, number of used features: 71\n",
      "[LightGBM] [Info] Number of data points in the train set: 108243, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080088 -> initscore=-2.441148\n",
      "[LightGBM] [Info] Start training from score -2.441148\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080088 -> initscore=-2.441148\n",
      "[LightGBM] [Info] Start training from score -2.441148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:59:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 12271, number of negative: 140253\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "Confusion matrix saved to ../images/model_performance/random_forest_confusion_matrix.png\n",
      "[LightGBM] [Info] Number of positive: 12271, number of negative: 140253\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3537\n",
      "[LightGBM] [Info] Number of data points in the train set: 152524, number of used features: 71\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3533\n",
      "[LightGBM] [Info] Number of data points in the train set: 152524, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080453 -> initscore=-2.436209\n",
      "[LightGBM] [Info] Start training from score -2.436209\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080453 -> initscore=-2.436209\n",
      "[LightGBM] [Info] Start training from score -2.436209\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3536\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "ROC curve saved to ../images/model_performance/random_forest_roc_curve.png\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.413919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3536\n",
      "Precision-Recall curve saved to ../images/model_performance/random_forest_pr_curve.png\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:59:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning curve saved to ../images/model_performance/decision_tree_learning_curve.png\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:59:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[19:59:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning curve saved to ../images/model_performance/xgboost_learning_curve.png\n",
      "============================================================\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1577, number of negative: 18103\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3474\n",
      "[LightGBM] [Info] Number of data points in the train set: 19680, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080132 -> initscore=-2.440553\n",
      "[LightGBM] [Info] Start training from score -2.440553\n",
      "[LightGBM] [Info] Number of positive: 5095, number of negative: 58866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3511\n",
      "[LightGBM] [Info] Number of data points in the train set: 63961, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.079658 -> initscore=-2.447004\n",
      "[LightGBM] [Info] Start training from score -2.447004\n",
      "[LightGBM] [Info] Number of positive: 8669, number of negative: 99574\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 108243, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080088 -> initscore=-2.441148\n",
      "[LightGBM] [Info] Start training from score -2.441148\n",
      "[LightGBM] [Info] Number of positive: 12271, number of negative: 140253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3535\n",
      "[LightGBM] [Info] Number of data points in the train set: 152524, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080453 -> initscore=-2.436209\n",
      "[LightGBM] [Info] Start training from score -2.436209\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3533\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "[LightGBM] [Info] Number of positive: 1577, number of negative: 18103\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3474\n",
      "[LightGBM] [Info] Number of data points in the train set: 19680, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080132 -> initscore=-2.440553\n",
      "[LightGBM] [Info] Start training from score -2.440553\n",
      "[LightGBM] [Info] Number of positive: 5095, number of negative: 58866\n",
      "[LightGBM] [Info] Number of positive: 8759, number of negative: 99484\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3516\n",
      "[LightGBM] [Info] Number of data points in the train set: 108243, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080920 -> initscore=-2.429915\n",
      "[LightGBM] [Info] Start training from score -2.429915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3511\n",
      "[LightGBM] [Info] Number of data points in the train set: 63961, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.079658 -> initscore=-2.447004\n",
      "[LightGBM] [Info] Start training from score -2.447004\n",
      "[LightGBM] [Info] Number of positive: 12271, number of negative: 140253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3527\n",
      "[LightGBM] [Info] Number of data points in the train set: 152524, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080453 -> initscore=-2.436209\n",
      "[LightGBM] [Info] Start training from score -2.436209\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Number of positive: 1577, number of negative: 18103\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3474\n",
      "[LightGBM] [Info] Number of data points in the train set: 19680, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080132 -> initscore=-2.440553\n",
      "[LightGBM] [Info] Start training from score -2.440553\n",
      "[LightGBM] [Info] Number of positive: 5095, number of negative: 58866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3511\n",
      "[LightGBM] [Info] Number of data points in the train set: 63961, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.079658 -> initscore=-2.447004\n",
      "[LightGBM] [Info] Start training from score -2.447004\n",
      "[LightGBM] [Info] Number of positive: 8759, number of negative: 99484\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3516\n",
      "[LightGBM] [Info] Number of data points in the train set: 108243, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080920 -> initscore=-2.429915\n",
      "[LightGBM] [Info] Start training from score -2.429915\n",
      "[LightGBM] [Info] Number of positive: 12265, number of negative: 140259\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 152524, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080414 -> initscore=-2.436741\n",
      "[LightGBM] [Info] Start training from score -2.436741\n",
      "[LightGBM] [Info] Number of positive: 15887, number of negative: 180919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3536\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080724 -> initscore=-2.432548\n",
      "[LightGBM] [Info] Start training from score -2.432548\n",
      "Learning curve saved to ../images/model_performance/lightgbm_learning_curve.png\n",
      "============================================================\n",
      "\n",
      "Learning curve saved to ../images/model_performance/random_forest_learning_curve.png\n",
      "============================================================\n",
      "\n",
      "Model performance comparison plot saved to ../images/model_performance/model_performance_comparison.png\n",
      "Combined confusion matrices plot saved to ../images/model_performance/combined_confusion_matrices.png\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    recall_score, precision_score, f1_score, roc_auc_score,\n",
    "    precision_recall_curve, auc, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "def train_evaluate_model(name, pipeline, X_train, y_train, X_val, y_val):\n",
    "    print(f\"Training {name}...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "    pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(f\"Confusion Matrix for {name}:\\n{cm}\\n\")\n",
    "\n",
    "    print(f\"{name} Metrics:\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC-PR: {pr_auc:.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc:.4f}\\n\")\n",
    "\n",
    "    os.makedirs(\"../images/model_performance/\", exist_ok=True)\n",
    "    base_path = f\"../images/model_performance/{name.lower().replace(' ', '_')}\"\n",
    "\n",
    "    # ROC Curve\n",
    "    save_path = f\"{base_path}_roc_curve.png\"\n",
    "    plot_roc_curve(y_val, y_pred_proba, save_path=save_path)\n",
    "    print(f\"ROC curve saved to {save_path}\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    save_path = f\"{base_path}_pr_curve.png\"\n",
    "    plot_precision_recall_curve(y_val, y_pred_proba, save_path=save_path)\n",
    "    print(f\"Precision-Recall curve saved to {save_path}\")\n",
    "\n",
    "    # Learning Curve\n",
    "    save_path = f\"{base_path}_learning_curve.png\"\n",
    "    plot_learning_curve(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        save_path=save_path\n",
    "    )\n",
    "    print(f\"Learning curve saved to {save_path}\")\n",
    "\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'pipeline': pipeline,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1,\n",
    "        'auc_pr': pr_auc,\n",
    "        'auc_roc': roc_auc\n",
    "    }\n",
    "\n",
    "# Define number of parallel jobs based on your CPU cores\n",
    "parallel_jobs = 6   # Adjust as needed\n",
    "\n",
    "# Train and evaluate all baseline models in parallel\n",
    "baseline_results = Parallel(n_jobs=parallel_jobs)(\n",
    "    delayed(train_evaluate_model)(name, pipeline, X_train, y_train, X_val, y_val)\n",
    "    for name, pipeline in baseline_models\n",
    ")\n",
    "\n",
    "# After all models have been trained and evaluated\n",
    "def plot_all_model_performance(results):\n",
    "    metrics = ['recall', 'precision', 'f1_score', 'auc_pr', 'auc_roc']\n",
    "    plot_model_performance(\n",
    "        {model['model']: {metric: model[metric] for metric in metrics} for model in results},\n",
    "        metrics,\n",
    "        save_path=\"../images/model_performance/model_performance_comparison.png\"\n",
    "    )\n",
    "    print(\"Model performance comparison plot saved to ../images/model_performance/model_performance_comparison.png\")\n",
    "\n",
    "def plot_all_confusion_matrices(results, y_val):\n",
    "    y_pred_dict = {model['model']: model['y_pred'] for model in results}\n",
    "    plot_combined_confusion_matrices(\n",
    "        {model['model']: {metric: model[metric] for metric in ['recall', 'precision', 'f1_score', 'auc_pr', 'auc_roc']} for model in results},\n",
    "        y_val,\n",
    "        y_pred_dict,\n",
    "        labels=[\"Non-Defaulter\", \"Defaulter\"],\n",
    "        save_path=\"../images/model_performance/combined_confusion_matrices.png\"\n",
    "    )\n",
    "    print(\"Combined confusion matrices plot saved to ../images/model_performance/combined_confusion_matrices.png\")\n",
    "\n",
    "# Generate combined plots\n",
    "plot_all_model_performance(baseline_results)\n",
    "plot_all_confusion_matrices(baseline_results, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define number of parallel jobs based on your CPU cores\n",
    "# parallel_jobs = 6   # Adjust as needed\n",
    "\n",
    "# # Train and evaluate all baseline models in parallel\n",
    "# baseline_results = Parallel(n_jobs=parallel_jobs)(\n",
    "#     delayed(train_evaluate_model)(name, pipeline, X_train, y_train, X_val, y_val)\n",
    "#     for name, pipeline in baseline_models\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipelines for different models.\n",
    "\n",
    "This way, we ensure the same preprocessing steps are applied within each model's evaluation during cross-validation, preventing data leakage and making results more reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameter grids for each model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

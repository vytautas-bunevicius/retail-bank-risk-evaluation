{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "This notebook builds upon the feature-engineered dataset from the previous notebook (`04_feature_engineering.ipynb`) and focuses on **Model Training and Evaluation**. Our primary goal is to develop a credit risk prediction model that excels at identifying potential loan defaulters, thereby minimizing financial losses for retail banks while also considering their desired balance between risk aversion and loan approval rates. This translates to maximizing the recall of the positive class (loan defaulters) while maintaining acceptable precision and overall model performance.\n",
    "\n",
    "### 0.5.1 Objectives\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "1. **Model Selection:** Choose algorithms suitable for imbalanced classification problems.\n",
    "2. **Model Training:** Train models with a focus on identifying potential defaulters.\n",
    "3. **Hyperparameter Tuning:** Optimize models to increase recall for the positive class.\n",
    "4. **Model Evaluation:** Assess models primarily on recall, while considering precision, F2-score, AUC-PR, and overall performance.\n",
    "5. **Model Comparison:** Compare different models based on their ability to identify true positives and balance the precision-recall trade-off.\n",
    "6. **Threshold Adjustment:** Explore the impact of classification thresholds on recall and precision, collaborating with retail banks to determine the optimal threshold.\n",
    "\n",
    "### 0.5.2 Importance of Focusing on Recall\n",
    "\n",
    "Prioritizing recall for defaulter prediction is crucial for minimizing financial losses, which is the primary business objective in credit risk assessment. The cost of missing a potential defaulter (false negative) is typically much higher than the cost of incorrectly classifying a non-defaulter as high-risk (false positive). While we prioritize recall, we will also carefully consider the precision-recall trade-off and aim for a model that maximizes recall without severely impacting precision. Techniques like threshold adjustment and cost-sensitive learning will be used to balance these metrics effectively. Furthermore, demonstrating a thorough approach to risk identification aligns with regulatory expectations in the financial sector, supporting the banks' compliance needs. This approach also allows for more conservative lending practices, which can be adjusted based on the bank's specific risk tolerance.\n",
    "\n",
    "### 0.5.3 Our Approach\n",
    "\n",
    "In this notebook, we will focus on the following modeling tasks:\n",
    "\n",
    "1. **Data Preparation:** Address class imbalance using techniques like SMOTE or class weighting.\n",
    "2. **Baseline Model:** A logistic regression model with class weights inversely proportional to class frequencies will serve as our baseline. This will provide a benchmark for evaluating more complex models.\n",
    "3. **Advanced Models:** Train and evaluate models known for handling imbalanced data:\n",
    "   - Decision Trees with adjusted class weights\n",
    "   - Random Forest with balanced class weights\n",
    "   - Gradient Boosting (XGBoost, LightGBM) with `scale_pos_weight` adjustment\n",
    "4. **Hyperparameter Tuning:** We will employ techniques like GridSearchCV or RandomizedSearchCV, optimizing for the F2-score (which gives more weight to recall) or a custom cost-sensitive scoring function.\n",
    "5. **Model Evaluation:** Prioritize recall in our metrics, while also considering precision, F2-score, AUC-PR, and AUC-ROC.\n",
    "6. **Threshold Adjustment:** We will experiment with different classification thresholds and work closely with retail banks to determine the optimal threshold that balances their desired level of risk aversion with acceptable loan approval rates.\n",
    "7. **Ensemble Methods:** Explore ensemble techniques that can improve recall without severely impacting precision.\n",
    "8. **Cost-Sensitive Learning:** Incorporate misclassification costs to reflect the higher cost of false negatives, aligning the model's objective with the business goal of minimizing financial losses.\n",
    "\n",
    "By the end of this notebook, we aim to have a model (or ensemble of models) that excels at identifying potential loan defaulters, providing the bank with a powerful tool for risk assessment and mitigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import optuna\n",
    "\n",
    "from retail_bank_risk.advanced_visualizations_utils import (\n",
    "    plot_combined_confusion_matrices,\n",
    "    plot_confusion_matrix,\n",
    "    plot_learning_curve,\n",
    "    plot_model_performance,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_roc_curve,\n",
    "    shap_force_plot,\n",
    "    shap_summary_plot,\n",
    ")\n",
    "from retail_bank_risk.model_training_utils import (\n",
    "    downscale_dtypes,\n",
    "    evaluate_model,\n",
    "    sanitize_feature_names,\n",
    "    optimize_hyperparameters\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (307511, 78)\n",
      "Test Data Shape: (48744, 77)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet(\"../data/processed/application_train_engineered.parquet\")\n",
    "test_df = pd.read_parquet(\"../data/processed/application_test_engineered.parquet\")\n",
    "\n",
    "print(f\"Training Data Shape: {train_df.shape}\")\n",
    "print(f\"Test Data Shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Data columns (total 78 columns):\n",
      " #   Column                                   Non-Null Count   Dtype  \n",
      "---  ------                                   --------------   -----  \n",
      " 0   reg_city_not_work_city_0                 307511 non-null  uint8  \n",
      " 1   reg_city_not_work_city_1                 307511 non-null  uint8  \n",
      " 2   region_rating_client_w_city              307511 non-null  float32\n",
      " 3   region_rating_client                     307511 non-null  float32\n",
      " 4   name_contract_type_cash loans            307511 non-null  uint8  \n",
      " 5   name_contract_type_revolving loans       307511 non-null  uint8  \n",
      " 6   code_gender_m                            307511 non-null  uint8  \n",
      " 7   code_gender_f                            307511 non-null  uint8  \n",
      " 8   flag_own_car_n                           307511 non-null  uint8  \n",
      " 9   flag_own_car_y                           307511 non-null  uint8  \n",
      " 10  flag_own_realty_y                        307511 non-null  uint8  \n",
      " 11  flag_own_realty_n                        307511 non-null  uint8  \n",
      " 12  name_type_suite_unaccompanied            307511 non-null  uint8  \n",
      " 13  name_type_suite_family                   307511 non-null  uint8  \n",
      " 14  name_type_suite_spouse, partner          307511 non-null  uint8  \n",
      " 15  name_type_suite_children                 307511 non-null  uint8  \n",
      " 16  name_type_suite_other_a                  307511 non-null  uint8  \n",
      " 17  name_type_suite_mode                     307511 non-null  uint8  \n",
      " 18  name_type_suite_other_b                  307511 non-null  uint8  \n",
      " 19  name_type_suite_group of people          307511 non-null  uint8  \n",
      " 20  name_income_type_working                 307511 non-null  uint8  \n",
      " 21  name_income_type_state servant           307511 non-null  uint8  \n",
      " 22  name_income_type_commercial associate    307511 non-null  uint8  \n",
      " 23  name_income_type_pensioner               307511 non-null  uint8  \n",
      " 24  name_income_type_unemployed              307511 non-null  uint8  \n",
      " 25  name_income_type_student                 307511 non-null  uint8  \n",
      " 26  name_income_type_businessman             307511 non-null  uint8  \n",
      " 27  name_income_type_maternity leave         307511 non-null  uint8  \n",
      " 28  name_education_type                      307511 non-null  float32\n",
      " 29  name_family_status_single / not married  307511 non-null  uint8  \n",
      " 30  name_family_status_married               307511 non-null  uint8  \n",
      " 31  name_family_status_civil marriage        307511 non-null  uint8  \n",
      " 32  name_family_status_widow                 307511 non-null  uint8  \n",
      " 33  name_family_status_separated             307511 non-null  uint8  \n",
      " 34  name_family_status_unknown               307511 non-null  uint8  \n",
      " 35  name_housing_type_house / apartment      307511 non-null  uint8  \n",
      " 36  name_housing_type_rented apartment       307511 non-null  uint8  \n",
      " 37  name_housing_type_with parents           307511 non-null  uint8  \n",
      " 38  name_housing_type_municipal apartment    307511 non-null  uint8  \n",
      " 39  name_housing_type_office apartment       307511 non-null  uint8  \n",
      " 40  name_housing_type_co-op apartment        307511 non-null  uint8  \n",
      " 41  occupation_type                          307511 non-null  float32\n",
      " 42  weekday_appr_process_start_wednesday     307511 non-null  uint8  \n",
      " 43  weekday_appr_process_start_monday        307511 non-null  uint8  \n",
      " 44  weekday_appr_process_start_thursday      307511 non-null  uint8  \n",
      " 45  weekday_appr_process_start_sunday        307511 non-null  uint8  \n",
      " 46  weekday_appr_process_start_saturday      307511 non-null  uint8  \n",
      " 47  weekday_appr_process_start_friday        307511 non-null  uint8  \n",
      " 48  weekday_appr_process_start_tuesday       307511 non-null  uint8  \n",
      " 49  organization_type                        307511 non-null  float32\n",
      " 50  housetype_mode_block of flats            307511 non-null  uint8  \n",
      " 51  housetype_mode_mode                      307511 non-null  uint8  \n",
      " 52  housetype_mode_terraced house            307511 non-null  uint8  \n",
      " 53  housetype_mode_specific housing          307511 non-null  uint8  \n",
      " 54  emergencystate_mode_no                   307511 non-null  uint8  \n",
      " 55  emergencystate_mode_mode                 307511 non-null  uint8  \n",
      " 56  emergencystate_mode_yes                  307511 non-null  uint8  \n",
      " 57  days_last_phone_change                   307511 non-null  float32\n",
      " 58  days_birth                               307511 non-null  float32\n",
      " 59  days_id_publish                          307511 non-null  float32\n",
      " 60  ext_source_3                             307511 non-null  float32\n",
      " 61  ext_source_2                             307511 non-null  float32\n",
      " 62  sk_id_curr                               307511 non-null  float32\n",
      " 63  amt_income_total                         307511 non-null  float32\n",
      " 64  amt_credit                               307511 non-null  float32\n",
      " 65  amt_annuity                              307511 non-null  float32\n",
      " 66  amt_goods_price                          307511 non-null  float32\n",
      " 67  is_anomaly_false                         307511 non-null  uint8  \n",
      " 68  is_anomaly_true                          307511 non-null  uint8  \n",
      " 69  age_group                                307511 non-null  float32\n",
      " 70  income_group                             307511 non-null  float32\n",
      " 71  credit_amount_group                      307511 non-null  float32\n",
      " 72  debt_to_income_ratio                     307511 non-null  float32\n",
      " 73  credit_to_goods_ratio                    307511 non-null  float32\n",
      " 74  annuity_to_income_ratio                  307511 non-null  float32\n",
      " 75  ext_source_mean                          307511 non-null  float32\n",
      " 76  credit_exceeds_goods                     307511 non-null  uint8  \n",
      " 77  target                                   307511 non-null  uint8  \n",
      "dtypes: float32(22), uint8(56)\n",
      "memory usage: 42.2 MB\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = downscale_dtypes(train_df, test_df, target_column='target')\n",
    "\n",
    "train_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (246008, 76)\n",
      "Validation set shape: (61503, 76)\n",
      "Test set shape: (48744, 76)\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop([\"target\", \"sk_id_curr\"], axis=1)\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_test = test_df.drop(\"sk_id_curr\", axis=1)\n",
    "sk_id_curr = test_df[\"sk_id_curr\"]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'Dummy Classifier': Pipeline([\n",
    "        ('sanitizer', FunctionTransformer(sanitize_feature_names)),\n",
    "        ('classifier', DummyClassifier(strategy='stratified', random_state=42))\n",
    "    ]),\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('sanitizer', FunctionTransformer(sanitize_feature_names)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectFromModel(LogisticRegression(random_state=42))),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced',\n",
    "                                          max_iter=1000, penalty='l2', C=0.1))\n",
    "    ]),\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('sanitizer', FunctionTransformer(sanitize_feature_names)),\n",
    "        ('feature_selection', SelectFromModel(DecisionTreeClassifier(random_state=42))),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42, class_weight='balanced',\n",
    "                                              max_depth=3, min_samples_split=5))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('sanitizer', FunctionTransformer(sanitize_feature_names)),\n",
    "        ('feature_selection', SelectFromModel(RandomForestClassifier(random_state=42))),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced',\n",
    "                                              n_jobs=1, max_depth=5, n_estimators=100,\n",
    "                                              min_samples_split=5, bootstrap=True))\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('sanitizer', FunctionTransformer(sanitize_feature_names)),\n",
    "        ('feature_selection', SelectFromModel(GradientBoostingClassifier(random_state=42))),\n",
    "        ('classifier', GradientBoostingClassifier(random_state=42, max_depth=3,\n",
    "                                                  n_estimators=100, learning_rate=0.01,\n",
    "                                                  subsample=0.8, min_samples_split=5))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('sanitizer', FunctionTransformer(sanitize_feature_names)),\n",
    "        ('feature_selection', SelectFromModel(xgb.XGBClassifier(random_state=42))),\n",
    "        ('classifier', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                                         random_state=42, scale_pos_weight=len(y)/sum(y),\n",
    "                                         max_depth=3, n_estimators=100, learning_rate=0.01,\n",
    "                                         subsample=0.8, colsample_bytree=0.8,\n",
    "                                         min_child_weight=5, n_jobs=-1))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('sanitizer', FunctionTransformer(sanitize_feature_names)),\n",
    "        ('feature_selection', SelectFromModel(lgb.LGBMClassifier(random_state=42))),\n",
    "        ('classifier', lgb.LGBMClassifier(random_state=42, class_weight='balanced',\n",
    "                                          max_depth=3, n_estimators=100, learning_rate=0.01,\n",
    "                                          subsample=0.8, colsample_bytree=0.8,\n",
    "                                          min_child_samples=5, n_jobs=-1))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Dummy Classifier...\n",
      "Loaded checkpoint: ../models/dummy_classifier_checkpoint.pkl\n",
      "Resumed from checkpoint for model Dummy Classifier.\n",
      "Dummy Classifier Cross-validation results:\n",
      "Precision: 0.0831\n",
      "Recall: 0.0822\n",
      "F1-Score: 0.0826\n",
      "AUC-ROC: 0.5013\n",
      "============================================================\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "Loaded checkpoint: ../models/logistic_regression_checkpoint.pkl\n",
      "Resumed from checkpoint for model Logistic Regression.\n",
      "Logistic Regression Cross-validation results:\n",
      "Precision: 0.1566\n",
      "Recall: 0.6663\n",
      "F1-Score: 0.2536\n",
      "AUC-ROC: 0.7372\n",
      "============================================================\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "Loaded checkpoint: ../models/decision_tree_checkpoint.pkl\n",
      "Resumed from checkpoint for model Decision Tree.\n",
      "Decision Tree Cross-validation results:\n",
      "Precision: 0.2039\n",
      "Recall: 0.6926\n",
      "F1-Score: 0.3151\n",
      "AUC-ROC: 0.7981\n",
      "============================================================\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Loaded checkpoint: ../models/random_forest_checkpoint.pkl\n",
      "Resumed from checkpoint for model Random Forest.\n",
      "Random Forest Cross-validation results:\n",
      "Precision: 0.2550\n",
      "Recall: 0.7633\n",
      "F1-Score: 0.3822\n",
      "AUC-ROC: 0.8873\n",
      "============================================================\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "Loaded checkpoint: ../models/gradient_boosting_checkpoint.pkl\n",
      "Resumed from checkpoint for model Gradient Boosting.\n",
      "Gradient Boosting Cross-validation results:\n",
      "Precision: 1.0000\n",
      "Recall: 0.1291\n",
      "F1-Score: 0.2287\n",
      "AUC-ROC: 0.8503\n",
      "============================================================\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Loaded checkpoint: ../models/xgboost_checkpoint.pkl\n",
      "Resumed from checkpoint for model XGBoost.\n",
      "XGBoost Cross-validation results:\n",
      "Precision: 0.3213\n",
      "Recall: 0.8750\n",
      "F1-Score: 0.4700\n",
      "AUC-ROC: 0.9448\n",
      "============================================================\n",
      "\n",
      "Evaluating LightGBM...\n",
      "Loaded checkpoint: ../models/lightgbm_checkpoint.pkl\n",
      "Resumed from checkpoint for model LightGBM.\n",
      "LightGBM Cross-validation results:\n",
      "Precision: 0.2963\n",
      "Recall: 0.8415\n",
      "F1-Score: 0.4383\n",
      "AUC-ROC: 0.9262\n",
      "============================================================\n",
      "\n",
      "Model Performance Ranking:\n",
      "\n",
      "Ranking by precision:\n",
      "1. Gradient Boosting: precision = 1.0000\n",
      "2. XGBoost: precision = 0.3213\n",
      "3. LightGBM: precision = 0.2963\n",
      "4. Random Forest: precision = 0.2550\n",
      "5. Decision Tree: precision = 0.2039\n",
      "6. Logistic Regression: precision = 0.1566\n",
      "7. Dummy Classifier: precision = 0.0831\n",
      "\n",
      "Ranking by recall:\n",
      "1. XGBoost: recall = 0.8750\n",
      "2. LightGBM: recall = 0.8415\n",
      "3. Random Forest: recall = 0.7633\n",
      "4. Decision Tree: recall = 0.6926\n",
      "5. Logistic Regression: recall = 0.6663\n",
      "6. Gradient Boosting: recall = 0.1291\n",
      "7. Dummy Classifier: recall = 0.0822\n",
      "\n",
      "Ranking by f1_score:\n",
      "1. XGBoost: f1_score = 0.4700\n",
      "2. LightGBM: f1_score = 0.4383\n",
      "3. Random Forest: f1_score = 0.3822\n",
      "4. Decision Tree: f1_score = 0.3151\n",
      "5. Logistic Regression: f1_score = 0.2536\n",
      "6. Gradient Boosting: f1_score = 0.2287\n",
      "7. Dummy Classifier: f1_score = 0.0826\n",
      "\n",
      "Ranking by auc_roc:\n",
      "1. XGBoost: auc_roc = 0.9448\n",
      "2. LightGBM: auc_roc = 0.9262\n",
      "3. Random Forest: auc_roc = 0.8873\n",
      "4. Gradient Boosting: auc_roc = 0.8503\n",
      "5. Decision Tree: auc_roc = 0.7981\n",
      "6. Logistic Regression: auc_roc = 0.7372\n",
      "7. Dummy Classifier: auc_roc = 0.5013\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, pipeline in pipelines.items():\n",
    "    result = evaluate_model(name, pipeline, X, y)\n",
    "    results.append(result)\n",
    "\n",
    "print(\"Model Performance Ranking:\")\n",
    "for metric in ['precision', 'recall', 'f1_score', 'auc_roc']:\n",
    "    print(f\"\\nRanking by {metric}:\")\n",
    "    sorted_results = sorted(results, key=lambda x: x[metric], reverse=True)\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {result['model']}: {metric} = {result[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated several machine learning models for credit risk prediction, initially prioritizing recall to minimize false negatives (missed defaulters). However, to obtain a more balanced performance that considers both minimizing false negatives and the impact of false positives (incorrectly rejecting good loan applications), we will optimize for the F2-score. The F2-score gives higher weight to recall, aligning with our aim to reduce financial losses from defaults while mitigating the negative effects of rejecting creditworthy applicants.\n",
    "\n",
    "**XGBoost** and **LightGBM** demonstrated the strongest initial performance, surpassing models like Random Forest, Gradient Boosting, Decision Trees, Logistic Regression, and the Dummy Classifier baseline. We'll concentrate our optimization efforts on XGBoost and LightGBM.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "1. **Hyperparameter Tuning with Optuna:** We'll employ Optuna to fine-tune XGBoost and LightGBM, seeking to maximize the F2-score.  Stratified K-Fold cross-validation will be crucial during optimization to provide robust performance estimates considering the dataset's class imbalance.\n",
    "\n",
    "2. **Model Training and Evaluation:** For each model (XGBoost and LightGBM):\n",
    "   a. Optimize hyperparameters via Optuna, targeting maximum F2.\n",
    "   b. Train the model with the optimal hyperparameters.\n",
    "   c. Evaluate on a held-out validation set using F2, precision, and recall.\n",
    "\n",
    "3. **Ensemble Methods (Conditional):** If optimized models perform similarly, we may explore ensemble methods to combine their strengths and potentially achieve greater predictive power.\n",
    "\n",
    "4. **Final Model Selection:** The final model will be chosen based on optimized performance, alignment with the bank's risk tolerance, and business objectives, considering the relative costs of false negatives and positives.\n",
    "\n",
    "\n",
    "This retains the original format and flow while incorporating all the crucial corrections and enhancements for clarity and accuracy regarding the F2-score optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed trials: 0\n",
      "Running 100 more trials to reach 100 in total.\n",
      "Loaded existing study 'xgboost_optimization'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 19:06:59,019] Trial 2 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.09983323972566299, 'n_estimators': 259, 'min_child_weight': 9, 'subsample': 0.997437890015122, 'colsample_bytree': 0.7500654138122846, 'gamma': 4.708616993349988e-08, 'scale_pos_weight': 13.174681179797952}. Best is trial 2 with value: 1.0.\n",
      "[I 2024-09-29 19:07:24,746] Trial 3 finished with value: 0.9937706559784537 and parameters: {'max_depth': 4, 'learning_rate': 0.019593970483689452, 'n_estimators': 776, 'min_child_weight': 1, 'subsample': 0.7109257340044541, 'colsample_bytree': 0.6913268006511722, 'gamma': 0.09475090323155133, 'scale_pos_weight': 1.9158086485222128}. Best is trial 2 with value: 1.0.\n",
      "[I 2024-09-29 19:07:25,122] Trial 0 finished with value: 0.9879848620623537 and parameters: {'max_depth': 9, 'learning_rate': 0.0069953888132409684, 'n_estimators': 186, 'min_child_weight': 2, 'subsample': 0.600266066900057, 'colsample_bytree': 0.7945266733158207, 'gamma': 0.07342366177962707, 'scale_pos_weight': 8.357213352033035}. Best is trial 2 with value: 1.0.\n",
      "[I 2024-09-29 19:07:42,043] Trial 1 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.04551205523283438, 'n_estimators': 300, 'min_child_weight': 5, 'subsample': 0.9828458094321034, 'colsample_bytree': 0.8654181129635092, 'gamma': 0.014898258524330391, 'scale_pos_weight': 6.994696889649713}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:08:02,889] Trial 7 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.1260994530084753, 'n_estimators': 990, 'min_child_weight': 2, 'subsample': 0.734104258720955, 'colsample_bytree': 0.9756903925856093, 'gamma': 4.2169351002230025e-07, 'scale_pos_weight': 87.2252814398808}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:08:13,478] Trial 5 finished with value: 0.9996132073547639 and parameters: {'max_depth': 9, 'learning_rate': 0.2683285562893714, 'n_estimators': 914, 'min_child_weight': 8, 'subsample': 0.7013021340262215, 'colsample_bytree': 0.9885802296291805, 'gamma': 0.0523730650710103, 'scale_pos_weight': 1.1729139678188822}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:08:27,156] Trial 6 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.20984067671914194, 'n_estimators': 270, 'min_child_weight': 2, 'subsample': 0.8324453776510538, 'colsample_bytree': 0.8525128728702824, 'gamma': 0.00035797369881504526, 'scale_pos_weight': 2.4523007089911637}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:08:37,410] Trial 9 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.07173120195129172, 'n_estimators': 766, 'min_child_weight': 5, 'subsample': 0.7672194176879873, 'colsample_bytree': 0.7738510811921142, 'gamma': 0.011679007620376707, 'scale_pos_weight': 12.853410493928028}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:08:47,267] Trial 8 finished with value: 0.9989684542275296 and parameters: {'max_depth': 8, 'learning_rate': 0.014026484736763895, 'n_estimators': 426, 'min_child_weight': 10, 'subsample': 0.854099010204406, 'colsample_bytree': 0.9740902293086788, 'gamma': 0.00014686210736628607, 'scale_pos_weight': 3.3514714417762876}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:08:49,644] Trial 4 finished with value: 0.9050629386615746 and parameters: {'max_depth': 9, 'learning_rate': 0.003583545717167744, 'n_estimators': 350, 'min_child_weight': 4, 'subsample': 0.9618952386402415, 'colsample_bytree': 0.8707220073224214, 'gamma': 0.4943231566058878, 'scale_pos_weight': 48.664456519531065}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:09:13,494] Trial 10 finished with value: 0.9985815550817232 and parameters: {'max_depth': 5, 'learning_rate': 0.019747028609706643, 'n_estimators': 774, 'min_child_weight': 10, 'subsample': 0.7457209860730174, 'colsample_bytree': 0.7155931652252463, 'gamma': 3.8184883809431176e-08, 'scale_pos_weight': 1.2673903240309234}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:09:22,337] Trial 11 finished with value: 0.9092025453855118 and parameters: {'max_depth': 9, 'learning_rate': 0.0035475344451549955, 'n_estimators': 445, 'min_child_weight': 6, 'subsample': 0.6698677952397453, 'colsample_bytree': 0.7899435571426209, 'gamma': 5.539329263096998e-08, 'scale_pos_weight': 54.31869252422792}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:09:35,030] Trial 13 finished with value: 0.6917619957267973 and parameters: {'max_depth': 6, 'learning_rate': 0.0011237437010116608, 'n_estimators': 242, 'min_child_weight': 6, 'subsample': 0.7605784111881558, 'colsample_bytree': 0.9189301709652047, 'gamma': 0.0003392913982648748, 'scale_pos_weight': 8.422194370561547}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:09:44,855] Trial 12 finished with value: 0.3665628690661724 and parameters: {'max_depth': 3, 'learning_rate': 0.001365841773675721, 'n_estimators': 946, 'min_child_weight': 3, 'subsample': 0.7469285140213264, 'colsample_bytree': 0.9393259196908685, 'gamma': 4.737470826414516e-08, 'scale_pos_weight': 54.521508254026585}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:09:53,345] Trial 14 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.07761371874448669, 'n_estimators': 580, 'min_child_weight': 1, 'subsample': 0.805236655505838, 'colsample_bytree': 0.6130983788204324, 'gamma': 0.11731206517168752, 'scale_pos_weight': 83.22746642538445}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:01,117] Trial 15 finished with value: 0.9990329729310912 and parameters: {'max_depth': 5, 'learning_rate': 0.03213138355706792, 'n_estimators': 453, 'min_child_weight': 6, 'subsample': 0.7972525975227556, 'colsample_bytree': 0.61462793986872, 'gamma': 0.035221815889845666, 'scale_pos_weight': 2.6984652618029648}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:05,446] Trial 16 finished with value: 1.0 and parameters: {'max_depth': 5, 'learning_rate': 0.07290514648474243, 'n_estimators': 383, 'min_child_weight': 6, 'subsample': 0.7435697602982739, 'colsample_bytree': 0.7963625534150665, 'gamma': 1.4233024604946956e-06, 'scale_pos_weight': 19.358472860826208}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:16,061] Trial 18 finished with value: 0.9982388108390066 and parameters: {'max_depth': 4, 'learning_rate': 0.07344753634876698, 'n_estimators': 315, 'min_child_weight': 5, 'subsample': 0.9216990372850027, 'colsample_bytree': 0.6155423022319758, 'gamma': 0.00014594105070528935, 'scale_pos_weight': 22.831328482790124}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:22,246] Trial 19 finished with value: 0.3051171182760547 and parameters: {'max_depth': 6, 'learning_rate': 0.0011544370745805758, 'n_estimators': 75, 'min_child_weight': 7, 'subsample': 0.9131747339292335, 'colsample_bytree': 0.600395847318527, 'gamma': 8.994632258543552e-06, 'scale_pos_weight': 27.022665444398754}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:25,977] Trial 20 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.8157908534641447, 'n_estimators': 61, 'min_child_weight': 7, 'subsample': 0.9845403117231314, 'colsample_bytree': 0.6190336176755531, 'gamma': 1.291735295379661e-06, 'scale_pos_weight': 15.538356629033617}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:31,958] Trial 21 finished with value: 0.9985588995925847 and parameters: {'max_depth': 7, 'learning_rate': 0.05896159830087992, 'n_estimators': 100, 'min_child_weight': 7, 'subsample': 0.9771870752129732, 'colsample_bytree': 0.6081597785028807, 'gamma': 4.643616768487385e-06, 'scale_pos_weight': 12.240940776783663}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:32,052] Trial 17 finished with value: 0.9996454628016964 and parameters: {'max_depth': 7, 'learning_rate': 0.011265411330739517, 'n_estimators': 693, 'min_child_weight': 6, 'subsample': 0.7459057892260831, 'colsample_bytree': 0.9011197455900699, 'gamma': 2.543016713629972e-07, 'scale_pos_weight': 5.297867938016313}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:34,373] Trial 22 finished with value: 0.9941908618636482 and parameters: {'max_depth': 7, 'learning_rate': 0.046158674347678785, 'n_estimators': 121, 'min_child_weight': 8, 'subsample': 0.9967094164615359, 'colsample_bytree': 0.6544855034452064, 'gamma': 3.8635946266951484e-06, 'scale_pos_weight': 19.247996484437884}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:38,242] Trial 24 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.9908745172668958, 'n_estimators': 68, 'min_child_weight': 8, 'subsample': 0.9835859056386453, 'colsample_bytree': 0.7323165840515047, 'gamma': 5.230353036253556e-06, 'scale_pos_weight': 19.72012725618674}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:39,667] Trial 23 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.8592886982014243, 'n_estimators': 137, 'min_child_weight': 8, 'subsample': 0.9898682588005244, 'colsample_bytree': 0.7255002483998376, 'gamma': 6.65918275955235e-06, 'scale_pos_weight': 18.18939709629811}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:42,060] Trial 25 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.5002092510427344, 'n_estimators': 72, 'min_child_weight': 8, 'subsample': 0.9910274213359735, 'colsample_bytree': 0.8530075153285408, 'gamma': 1.9238563443203927e-06, 'scale_pos_weight': 19.025566307012195}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:43,510] Trial 26 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.6877708143267169, 'n_estimators': 59, 'min_child_weight': 8, 'subsample': 0.9848311996609678, 'colsample_bytree': 0.721178816525709, 'gamma': 5.949240227802724e-06, 'scale_pos_weight': 4.856065403420868}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:10:48,255] Trial 27 finished with value: 0.9999677731227845 and parameters: {'max_depth': 7, 'learning_rate': 0.9295247248492053, 'n_estimators': 153, 'min_child_weight': 8, 'subsample': 0.9980879331737278, 'colsample_bytree': 0.7329330692809887, 'gamma': 0.0034216082163324884, 'scale_pos_weight': 5.601551883881949}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:04,370] Trial 28 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.5752702207776959, 'n_estimators': 600, 'min_child_weight': 9, 'subsample': 0.9277687400592688, 'colsample_bytree': 0.7350276171422365, 'gamma': 0.004243046265466782, 'scale_pos_weight': 5.2952597705032}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:08,738] Trial 29 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.5460947461133805, 'n_estimators': 577, 'min_child_weight': 8, 'subsample': 0.9971745623151115, 'colsample_bytree': 0.8463323890520099, 'gamma': 0.0036458866476267358, 'scale_pos_weight': 4.921607513126483}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:13,905] Trial 30 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.39797188664608774, 'n_estimators': 202, 'min_child_weight': 9, 'subsample': 0.8840063069157011, 'colsample_bytree': 0.8708894802952083, 'gamma': 0.0010306936207089088, 'scale_pos_weight': 5.179935105396944}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:19,640] Trial 31 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.22085602874002852, 'n_estimators': 234, 'min_child_weight': 9, 'subsample': 0.8812913460048097, 'colsample_bytree': 0.8453361749182833, 'gamma': 0.002159582207555361, 'scale_pos_weight': 4.906449803392084}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:26,866] Trial 32 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.21913592353176542, 'n_estimators': 248, 'min_child_weight': 4, 'subsample': 0.9216457030608765, 'colsample_bytree': 0.8470390030104832, 'gamma': 0.0025196023768477497, 'scale_pos_weight': 5.531891026008668}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:32,613] Trial 33 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.22518436281266024, 'n_estimators': 270, 'min_child_weight': 4, 'subsample': 0.924982373696263, 'colsample_bytree': 0.8456675557054935, 'gamma': 0.0036201980661194942, 'scale_pos_weight': 5.477039298229364}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:39,121] Trial 34 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.23033622835242457, 'n_estimators': 277, 'min_child_weight': 4, 'subsample': 0.9270476238988492, 'colsample_bytree': 0.8457675983644958, 'gamma': 0.0018543374478814918, 'scale_pos_weight': 4.996162814538079}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:45,372] Trial 35 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.17436059400718862, 'n_estimators': 263, 'min_child_weight': 4, 'subsample': 0.9289940921684878, 'colsample_bytree': 0.8340865167237997, 'gamma': 0.002064032131446179, 'scale_pos_weight': 4.805200849488105}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:51,108] Trial 36 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.18724374093598403, 'n_estimators': 250, 'min_child_weight': 4, 'subsample': 0.9317595952423766, 'colsample_bytree': 0.8327106776352308, 'gamma': 0.0013498147778653444, 'scale_pos_weight': 6.626081851245571}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:11:55,143] Trial 37 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.3247554000923005, 'n_estimators': 266, 'min_child_weight': 4, 'subsample': 0.932737169055933, 'colsample_bytree': 0.8337836627883244, 'gamma': 0.0009702864371057655, 'scale_pos_weight': 3.1879540673065137}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:12:04,044] Trial 38 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.15887672114332355, 'n_estimators': 278, 'min_child_weight': 3, 'subsample': 0.8836931340588812, 'colsample_bytree': 0.8367038890433811, 'gamma': 0.0010693033134292075, 'scale_pos_weight': 8.463698927373208}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:12:07,637] Trial 39 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.1804965845218498, 'n_estimators': 280, 'min_child_weight': 3, 'subsample': 0.8579326820825873, 'colsample_bytree': 0.8491588376221836, 'gamma': 4.210978799696528e-05, 'scale_pos_weight': 3.2197487109465284}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:12:13,130] Trial 40 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.1635504860436189, 'n_estimators': 264, 'min_child_weight': 2, 'subsample': 0.6389410899071412, 'colsample_bytree': 0.8247936360784467, 'gamma': 1.2664466331722863e-08, 'scale_pos_weight': 33.00218583403158}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:12:19,556] Trial 41 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.14353546547088597, 'n_estimators': 252, 'min_child_weight': 2, 'subsample': 0.6243622428209116, 'colsample_bytree': 0.81969847844507, 'gamma': 2.3175171894129673e-07, 'scale_pos_weight': 1.8215179477774157}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:12:25,686] Trial 42 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.1277419395659023, 'n_estimators': 297, 'min_child_weight': 2, 'subsample': 0.9451062476195303, 'colsample_bytree': 0.8239127642911637, 'gamma': 2.5427321398703633e-05, 'scale_pos_weight': 33.31280426784935}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:12:33,875] Trial 43 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.12302200710575492, 'n_estimators': 305, 'min_child_weight': 2, 'subsample': 0.6090999018684577, 'colsample_bytree': 0.7637748428064469, 'gamma': 2.9383159000262548e-05, 'scale_pos_weight': 33.96362126145855}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:13:00,736] Trial 44 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.12648489298676538, 'n_estimators': 868, 'min_child_weight': 2, 'subsample': 0.6245421716757273, 'colsample_bytree': 0.7629607519196602, 'gamma': 1.0150569012564056e-08, 'scale_pos_weight': 1.8597819909020292}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:13:08,072] Trial 45 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.10977803258100942, 'n_estimators': 840, 'min_child_weight': 2, 'subsample': 0.6370771823349406, 'colsample_bytree': 0.9620359584741258, 'gamma': 4.652594577760283e-05, 'scale_pos_weight': 1.6101628496109834}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:13:37,117] Trial 46 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.034486823513569194, 'n_estimators': 857, 'min_child_weight': 2, 'subsample': 0.6398066380248446, 'colsample_bytree': 0.9611042178594136, 'gamma': 1.096212275566741e-08, 'scale_pos_weight': 1.7657660471065966}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:13:42,363] Trial 47 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.12130979841913199, 'n_estimators': 850, 'min_child_weight': 2, 'subsample': 0.6085353726666507, 'colsample_bytree': 0.7587835979200092, 'gamma': 1.0630176349915875e-08, 'scale_pos_weight': 1.975662590397382}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:14:19,486] Trial 48 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.12824949004857125, 'n_estimators': 840, 'min_child_weight': 2, 'subsample': 0.6352940242170841, 'colsample_bytree': 0.9638259506540403, 'gamma': 3.0447607008540442e-05, 'scale_pos_weight': 1.5539468918979185}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:14:27,172] Trial 49 finished with value: 1.0 and parameters: {'max_depth': 8, 'learning_rate': 0.10973489655723381, 'n_estimators': 878, 'min_child_weight': 2, 'subsample': 0.7136401015301552, 'colsample_bytree': 0.9721350009555635, 'gamma': 1.7682059599818588e-08, 'scale_pos_weight': 1.5535012848328245}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:15:02,529] Trial 50 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.12020182552329323, 'n_estimators': 838, 'min_child_weight': 1, 'subsample': 0.7039802824631375, 'colsample_bytree': 0.9670604567941541, 'gamma': 0.012714179714155662, 'scale_pos_weight': 1.6594410104633084}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:15:22,116] Trial 51 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.026338256434222062, 'n_estimators': 857, 'min_child_weight': 1, 'subsample': 0.7093286126744662, 'colsample_bytree': 0.7710069329075024, 'gamma': 0.01575446368184146, 'scale_pos_weight': 10.942530622613761}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:15:30,706] Trial 52 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.10142412825129082, 'n_estimators': 897, 'min_child_weight': 5, 'subsample': 0.6992504070701198, 'colsample_bytree': 0.7761199809174203, 'gamma': 0.4466090724132749, 'scale_pos_weight': 89.51701153340008}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:16:13,599] Trial 53 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.03144766844311884, 'n_estimators': 995, 'min_child_weight': 1, 'subsample': 0.6979301455633907, 'colsample_bytree': 0.9602390964477345, 'gamma': 0.014293036502586115, 'scale_pos_weight': 11.523095368512898}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:16:21,135] Trial 54 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.027840279560891808, 'n_estimators': 887, 'min_child_weight': 5, 'subsample': 0.7038915167220539, 'colsample_bytree': 0.9959210225107368, 'gamma': 0.02147940842639604, 'scale_pos_weight': 11.714888897422508}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:16:27,041] Trial 55 finished with value: 0.9995487717836014 and parameters: {'max_depth': 9, 'learning_rate': 0.026788827511384505, 'n_estimators': 357, 'min_child_weight': 5, 'subsample': 0.710703187493792, 'colsample_bytree': 0.886388672574659, 'gamma': 0.025264459627300687, 'scale_pos_weight': 1.0172649187932905}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:17:32,029] Trial 56 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.02458442311174449, 'n_estimators': 978, 'min_child_weight': 5, 'subsample': 0.7104431830541091, 'colsample_bytree': 0.8863851641065613, 'gamma': 0.041659723842098415, 'scale_pos_weight': 11.579530949749643}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:17:39,791] Trial 57 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.022679978689215754, 'n_estimators': 995, 'min_child_weight': 1, 'subsample': 0.7774040783847773, 'colsample_bytree': 0.9976831344174989, 'gamma': 0.017291092609457062, 'scale_pos_weight': 97.28787162109228}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:17:46,283] Trial 58 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.021516872502509103, 'n_estimators': 993, 'min_child_weight': 1, 'subsample': 0.7843517225505883, 'colsample_bytree': 0.8876122204356749, 'gamma': 0.020749047449014568, 'scale_pos_weight': 13.07133110801388}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:18:12,951] Trial 60 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.08055805124207352, 'n_estimators': 987, 'min_child_weight': 1, 'subsample': 0.7991655004203077, 'colsample_bytree': 0.9006977553147418, 'gamma': 0.2676427961831035, 'scale_pos_weight': 91.77084359519098}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:18:28,034] Trial 59 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.024310616246938924, 'n_estimators': 981, 'min_child_weight': 5, 'subsample': 0.7885529739936802, 'colsample_bytree': 0.8944530308875129, 'gamma': 0.025580211538343018, 'scale_pos_weight': 12.730750454751291}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:18:33,115] Trial 61 finished with value: 0.9998711203963448 and parameters: {'max_depth': 9, 'learning_rate': 0.020599431715894047, 'n_estimators': 384, 'min_child_weight': 1, 'subsample': 0.7812051623460619, 'colsample_bytree': 0.8949566856533328, 'gamma': 0.12173588845489557, 'scale_pos_weight': 86.74331675238857}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:18:37,341] Trial 62 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.07016022378509433, 'n_estimators': 712, 'min_child_weight': 1, 'subsample': 0.7975580634471376, 'colsample_bytree': 0.6713342989212706, 'gamma': 0.10783836614791383, 'scale_pos_weight': 60.411473439526816}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:18:55,892] Trial 64 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.07143594132643667, 'n_estimators': 374, 'min_child_weight': 3, 'subsample': 0.7968657202252302, 'colsample_bytree': 0.6879623600847992, 'gamma': 0.10608674702348586, 'scale_pos_weight': 74.91355298833682}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:18:55,960] Trial 63 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.07462234171003129, 'n_estimators': 734, 'min_child_weight': 3, 'subsample': 0.7952831800197673, 'colsample_bytree': 0.6893413928013686, 'gamma': 0.14286823027948864, 'scale_pos_weight': 95.38800601451055}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:19:19,551] Trial 65 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.07623865796931098, 'n_estimators': 713, 'min_child_weight': 3, 'subsample': 0.7970885326002448, 'colsample_bytree': 0.6866331108596614, 'gamma': 0.12050374236001342, 'scale_pos_weight': 92.25027673993068}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:19:20,518] Trial 66 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.08031864031296007, 'n_estimators': 687, 'min_child_weight': 1, 'subsample': 0.7954654275566752, 'colsample_bytree': 0.6874966101377933, 'gamma': 0.32460849827515914, 'scale_pos_weight': 87.53552004969265}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:19:45,865] Trial 68 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.07358823273801121, 'n_estimators': 686, 'min_child_weight': 3, 'subsample': 0.8253317138404568, 'colsample_bytree': 0.6900318806308936, 'gamma': 0.1310167110151224, 'scale_pos_weight': 54.11447573966737}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:19:46,114] Trial 67 finished with value: 1.0 and parameters: {'max_depth': 9, 'learning_rate': 0.07783903938332486, 'n_estimators': 715, 'min_child_weight': 3, 'subsample': 0.818059141581074, 'colsample_bytree': 0.6679383080655523, 'gamma': 0.10075138327537415, 'scale_pos_weight': 66.74242560950461}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:19:57,095] Trial 70 finished with value: 0.9672749821915938 and parameters: {'max_depth': 3, 'learning_rate': 0.05297939316175826, 'n_estimators': 388, 'min_child_weight': 6, 'subsample': 0.8150131389160344, 'colsample_bytree': 0.6911828064918574, 'gamma': 0.1255880148831738, 'scale_pos_weight': 49.9160396140609}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:20:05,911] Trial 69 finished with value: 0.999170943197391 and parameters: {'max_depth': 4, 'learning_rate': 0.04134885923166729, 'n_estimators': 719, 'min_child_weight': 3, 'subsample': 0.8230900027793571, 'colsample_bytree': 0.6945500445903875, 'gamma': 0.10416579087957778, 'scale_pos_weight': 70.47472732546208}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:20:20,248] Trial 71 finished with value: 1.0 and parameters: {'max_depth': 5, 'learning_rate': 0.04935226301309297, 'n_estimators': 710, 'min_child_weight': 6, 'subsample': 0.819881404028644, 'colsample_bytree': 0.686475917725764, 'gamma': 2.1168167386639922e-07, 'scale_pos_weight': 74.6872135302851}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:20:23,487] Trial 72 finished with value: 1.0 and parameters: {'max_depth': 5, 'learning_rate': 0.05058310816704504, 'n_estimators': 524, 'min_child_weight': 6, 'subsample': 0.8234923232325763, 'colsample_bytree': 0.8037242917571749, 'gamma': 1.0365159168137556e-07, 'scale_pos_weight': 71.78613957463308}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:20:37,499] Trial 73 finished with value: 1.0 and parameters: {'max_depth': 5, 'learning_rate': 0.05130268981298419, 'n_estimators': 511, 'min_child_weight': 10, 'subsample': 0.733148628889029, 'colsample_bytree': 0.8079447235198584, 'gamma': 3.0330913895253765e-07, 'scale_pos_weight': 15.356571377572427}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:20:41,954] Trial 74 finished with value: 1.0 and parameters: {'max_depth': 5, 'learning_rate': 0.04744357417717875, 'n_estimators': 505, 'min_child_weight': 10, 'subsample': 0.8171444859655638, 'colsample_bytree': 0.933958128938481, 'gamma': 1.7930251252076044e-07, 'scale_pos_weight': 15.883269968260489}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:20:55,533] Trial 75 finished with value: 0.9999919439297511 and parameters: {'max_depth': 5, 'learning_rate': 0.04665029438951031, 'n_estimators': 484, 'min_child_weight': 10, 'subsample': 0.835232879091882, 'colsample_bytree': 0.8009853362066043, 'gamma': 1.8026692008036573e-07, 'scale_pos_weight': 15.586553678683282}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:00,850] Trial 76 finished with value: 1.0 and parameters: {'max_depth': 6, 'learning_rate': 0.045050437332251246, 'n_estimators': 489, 'min_child_weight': 10, 'subsample': 0.7354669471597773, 'colsample_bytree': 0.7976470662971983, 'gamma': 2.0824237142609899e-07, 'scale_pos_weight': 42.0858659221927}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:18,266] Trial 77 finished with value: 0.9988898047216834 and parameters: {'max_depth': 4, 'learning_rate': 0.047669886203659945, 'n_estimators': 513, 'min_child_weight': 10, 'subsample': 0.7325264330555473, 'colsample_bytree': 0.8040532854519329, 'gamma': 1.794228478193177e-07, 'scale_pos_weight': 44.01631277482334}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:26,086] Trial 78 finished with value: 1.0 and parameters: {'max_depth': 5, 'learning_rate': 0.04254261351411525, 'n_estimators': 504, 'min_child_weight': 10, 'subsample': 0.7608600063514168, 'colsample_bytree': 0.8036732331766719, 'gamma': 3.367925830665168e-07, 'scale_pos_weight': 16.275371375393895}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:39,634] Trial 79 finished with value: 0.9999033167718195 and parameters: {'max_depth': 4, 'learning_rate': 0.042584295446803984, 'n_estimators': 538, 'min_child_weight': 10, 'subsample': 0.760537005601065, 'colsample_bytree': 0.8014279883895837, 'gamma': 4.819244824576131e-07, 'scale_pos_weight': 9.434335434495544}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:43,289] Trial 80 finished with value: 1.0 and parameters: {'max_depth': 5, 'learning_rate': 0.32213056459102657, 'n_estimators': 455, 'min_child_weight': 7, 'subsample': 0.960270963469112, 'colsample_bytree': 0.6376924861548379, 'gamma': 9.849781633548098e-07, 'scale_pos_weight': 8.951453550007239}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:46,870] Trial 81 finished with value: 0.8888899318640006 and parameters: {'max_depth': 6, 'learning_rate': 0.013217300698022145, 'n_estimators': 174, 'min_child_weight': 7, 'subsample': 0.758618300083977, 'colsample_bytree': 0.6451938310647912, 'gamma': 9.354948674430878e-07, 'scale_pos_weight': 17.332752293982704}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:49,533] Trial 82 finished with value: 1.0 and parameters: {'max_depth': 6, 'learning_rate': 0.3027014266770815, 'n_estimators': 170, 'min_child_weight': 7, 'subsample': 0.9663052896789146, 'colsample_bytree': 0.6270276769959704, 'gamma': 1.9837417726421205e-06, 'scale_pos_weight': 14.37815156338514}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:21:55,163] Trial 83 finished with value: 0.9258133527961008 and parameters: {'max_depth': 6, 'learning_rate': 0.013678065154553625, 'n_estimators': 166, 'min_child_weight': 7, 'subsample': 0.9712307341138321, 'colsample_bytree': 0.6310623485249734, 'gamma': 1.1291554503568193e-06, 'scale_pos_weight': 9.071256036893963}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:00,434] Trial 84 finished with value: 0.8259926012124641 and parameters: {'max_depth': 6, 'learning_rate': 0.012101250436397893, 'n_estimators': 168, 'min_child_weight': 7, 'subsample': 0.9629810242198902, 'colsample_bytree': 0.6426026143428983, 'gamma': 7.149315316726012e-07, 'scale_pos_weight': 23.556063922483922}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:02,553] Trial 85 finished with value: 1.0 and parameters: {'max_depth': 6, 'learning_rate': 0.37587140416706916, 'n_estimators': 171, 'min_child_weight': 7, 'subsample': 0.9577351073217644, 'colsample_bytree': 0.6354241278296887, 'gamma': 8.868703369977161e-07, 'scale_pos_weight': 24.06660540913366}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:06,916] Trial 86 finished with value: 0.9999677770169573 and parameters: {'max_depth': 6, 'learning_rate': 0.09352514237502312, 'n_estimators': 193, 'min_child_weight': 7, 'subsample': 0.9646821297497958, 'colsample_bytree': 0.6290888751583366, 'gamma': 5.969513115601526e-07, 'scale_pos_weight': 24.058878268771743}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:16,856] Trial 87 finished with value: 0.9933519948614767 and parameters: {'max_depth': 10, 'learning_rate': 0.013824644136204644, 'n_estimators': 189, 'min_child_weight': 7, 'subsample': 0.9728420652490279, 'colsample_bytree': 0.6309063520496172, 'gamma': 8.811660344304585e-07, 'scale_pos_weight': 23.29368648858375}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:17,939] Trial 88 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.2932057948313617, 'n_estimators': 201, 'min_child_weight': 7, 'subsample': 0.9674820277231099, 'colsample_bytree': 0.6398156391107581, 'gamma': 1.0697751619148694e-06, 'scale_pos_weight': 23.798624322525413}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:25,530] Trial 90 finished with value: 1.0 and parameters: {'max_depth': 6, 'learning_rate': 0.8845480042872323, 'n_estimators': 171, 'min_child_weight': 7, 'subsample': 0.9762198177887362, 'colsample_bytree': 0.6256314551316979, 'gamma': 1.0213277450665946e-05, 'scale_pos_weight': 6.750339621223188}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:30,711] Trial 91 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.8604698095826517, 'n_estimators': 98, 'min_child_weight': 9, 'subsample': 0.9711496260216247, 'colsample_bytree': 0.7461272167009277, 'gamma': 0.0005129921455452183, 'scale_pos_weight': 23.076990408191815}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:35,729] Trial 89 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.09730784499653203, 'n_estimators': 419, 'min_child_weight': 7, 'subsample': 0.9695296625502985, 'colsample_bytree': 0.6364203483182639, 'gamma': 1.1498860071658367e-06, 'scale_pos_weight': 22.588808040123105}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:38,798] Trial 92 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.6628000877420965, 'n_estimators': 213, 'min_child_weight': 9, 'subsample': 0.9842767864533796, 'colsample_bytree': 0.7481030714096255, 'gamma': 0.0004024149482985214, 'scale_pos_weight': 25.433318203228737}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:40,216] Trial 93 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.6195854485553524, 'n_estimators': 213, 'min_child_weight': 9, 'subsample': 0.9496229244413159, 'colsample_bytree': 0.7430728067636115, 'gamma': 0.00020095327766308051, 'scale_pos_weight': 7.403688529950923}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:43,461] Trial 94 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.4764561769960563, 'n_estimators': 125, 'min_child_weight': 9, 'subsample': 0.9894226020612364, 'colsample_bytree': 0.7406643475824823, 'gamma': 1.04720011478876e-05, 'scale_pos_weight': 6.919283649090821}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:45,067] Trial 95 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.6652725665368027, 'n_estimators': 220, 'min_child_weight': 9, 'subsample': 0.9840620698645622, 'colsample_bytree': 0.7493452194427921, 'gamma': 0.9172835469160886, 'scale_pos_weight': 3.8040507168122675}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:50,965] Trial 97 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.8139019115359951, 'n_estimators': 130, 'min_child_weight': 8, 'subsample': 0.9789482613136414, 'colsample_bytree': 0.752608418706393, 'gamma': 0.007458523842515585, 'scale_pos_weight': 7.298625617308504}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:54,998] Trial 96 finished with value: 1.0 and parameters: {'max_depth': 10, 'learning_rate': 0.6867361781763923, 'n_estimators': 330, 'min_child_weight': 9, 'subsample': 0.9828947442173971, 'colsample_bytree': 0.7848889791097512, 'gamma': 1.0306779895475072e-05, 'scale_pos_weight': 7.187051573094308}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:56,539] Trial 98 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.7790103940541478, 'n_estimators': 139, 'min_child_weight': 8, 'subsample': 0.9899076872582867, 'colsample_bytree': 0.7478111356305509, 'gamma': 0.000454688207155397, 'scale_pos_weight': 6.98908915738194}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-09-29 19:22:57,855] Trial 99 finished with value: 1.0 and parameters: {'max_depth': 7, 'learning_rate': 0.8180033994469965, 'n_estimators': 94, 'min_child_weight': 9, 'subsample': 0.9063561587257962, 'colsample_bytree': 0.7475948618864252, 'gamma': 0.00031865814814723394, 'scale_pos_weight': 2.6248618088974407}. Best is trial 1 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at: ../models/xgboost_trial_1_checkpoint.pkl\n",
      "Best XGBoost parameters: {'max_depth': 10, 'learning_rate': 0.04551205523283438, 'n_estimators': 300, 'min_child_weight': 5, 'subsample': 0.9828458094321034, 'colsample_bytree': 0.8654181129635092, 'gamma': 0.014898258524330391, 'scale_pos_weight': 6.994696889649713}\n",
      "Best XGBoost F2 score: 1.0\n",
      "Best XGBoost model loaded from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "storage = \"sqlite:///../data/optuna_study.db\"\n",
    "study = optuna.load_study(study_name=\"xgboost_optimization\", storage=storage)\n",
    "\n",
    "completed_trials = len(study.trials)\n",
    "print(f\"Number of completed trials: {completed_trials}\")\n",
    "\n",
    "remaining_trials = 100 - completed_trials\n",
    "if remaining_trials > 0:\n",
    "    print(f\"Running {remaining_trials} more trials to reach 100 in total.\")\n",
    "\n",
    "    best_params_xgb, best_f2_score_xgb, best_model_xgb = optimize_hyperparameters(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        model_type='xgboost',\n",
    "        n_trials=remaining_trials,\n",
    "        n_jobs=-1,\n",
    "        checkpoint_dir='../models',\n",
    "        study_name=\"xgboost_optimization\",\n",
    "        storage=storage\n",
    "    )\n",
    "\n",
    "    print(f\"Best XGBoost parameters: {best_params_xgb}\")\n",
    "    print(f\"Best XGBoost F2 score: {best_f2_score_xgb}\")\n",
    "    print(\"Best XGBoost model loaded from checkpoint.\")\n",
    "else:\n",
    "    print(\"Study has already completed 100 or more trials.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed trials for LightGBM: 0\n",
      "Running 100 more trials for LightGBM to reach 100 in total.\n",
      "Loaded existing study 'lightgbm_optimization'.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.load_study(study_name=\"lightgbm_optimization\", storage=storage)\n",
    "\n",
    "completed_trials = len(study.trials)\n",
    "print(f\"Number of completed trials for LightGBM: {completed_trials}\")\n",
    "\n",
    "remaining_trials = 100 - completed_trials\n",
    "if remaining_trials > 0:\n",
    "    print(f\"Running {remaining_trials} more trials for LightGBM to reach 100 in total.\")\n",
    "\n",
    "    best_params_lgb, best_f2_score_lgb, best_model_lgb = optimize_hyperparameters(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        model_type='lightgbm',\n",
    "        n_trials=remaining_trials,\n",
    "        n_jobs=-1,\n",
    "        checkpoint_dir='../models',\n",
    "        study_name=\"lightgbm_optimization\",\n",
    "        storage=storage\n",
    "    )\n",
    "\n",
    "    print(f\"Best LightGBM parameters: {best_params_lgb}\")\n",
    "    print(f\"Best LightGBM F2 score: {best_f2_score_lgb}\")\n",
    "    print(\"Best LightGBM model loaded from checkpoint.\")\n",
    "else:\n",
    "    print(\"Study has already completed 100 or more trials for LightGBM.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
